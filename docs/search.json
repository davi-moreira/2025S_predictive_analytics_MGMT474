[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidesbook lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidesbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidesbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslidesbook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023- Book Chapter: Modern Dive -Bootstrapping and Confidence Intervals\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidesbook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslides - TBPbook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslides - TBPbook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nSupport Vector Machines\nCh. 9\nslides - TBPbook lab\n- Video: Statistical Learning: 9.1 Optimal Separating Hyperplane- Video: Statistical Learning: 9.2.Support Vector Classifier- Video: Statistical Learning: 9.3 Feature Expansion and the SVM- Video: Statistical Learning: 9.4 Example and Comparison with Logistic Regression- Video: Statistical Learning: 9.Py Support Vector Machines I 2023- Video: Statistical Learning: 9.Py ROC Curves I 2023\n\n\nWeek 09\nUnsupervised Learning\nCh. 12\nslides - TBPbook lab\n- Video: Statistical Learning: 12.1 Principal Components- Video: Statistical Learning: 12.2 Higher order principal components- Video: Statistical Learning: 12.3 k means Clustering- Video: Statistical Learning: 12.4 Hierarchical Clustering- Video: Statistical Learning: 12.5 Matrix Completion- Video: Statistical Learning: 12.6 Breast Cancer Example- Video: Statistical Learning: 12.Py Principal Components I 2023- Video: Statistical Learning: 12.Py Clustering I 2023- Video: Statistical Learning: 12.Py Application: NCI60 Data I 2023\n\n\nWeek 10\nFinal Project\n.\n.\n.\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nDeep Learning\nCh. 10\nslides - TBPbook lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 14\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\n\n* The course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nStructured Data\nUnstructured Data\nDatabases\nRelational Databases\n\n\n\nNon Relational Databases\nMeta Data and Dictionary (code book)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is Data?",
    "text": "What is Data?\n\n\n\n\nData refers to raw, unprocessed facts, figures, and symbols that represent information about the world around us. Data can take many forms, such as numbers, text, images, audio, and video, and it can be quantitative (numerical) or qualitative (categorical).\n\n\n\nData (Wiki)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Types of Data",
    "text": "Types of Data\n\n\nStructured Data: Data that is organized in a defined format, such as rows and columns in a database (e.g., an Excel spreadsheet).\nUnstructured Data: Data that does not have a predefined structure, such as text, emails, social media posts, videos, and images.\nSemi-Structured Data: Data that does not conform to a strict structure but contains tags or markers to separate elements (e.g., XML or JSON files)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Structured Business Data",
    "text": "Structured Business Data\n\n\nBusiness data refers to the information gathered by an organization, such as customer data, financial data, sales data, employee data, and more. Business data can come from a wide variety of sources - from customers’ purchase transactions and social media activities to market research and financial reports.\n\nBecause structured data is typically organized in a specific format that can be easily searched and analyzed, most business analytics are designed and applied to structured data. This course will focus solely on using and analyzing structured data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Introduction to Databases",
    "text": "Introduction to Databases\n\n\n\n\nA Database is a structured collection of data, typically managed by a Database Management System (DBMS) to efficiently store, retrieve, and manage data for various applications."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Relational Model: 1970s",
    "text": "The Relational Model: 1970s\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdgar F. Codd\n\n\n\n\nEdgar F. Codd: Proposed the relational model in 1970, which became the foundation for modern databases.\nRDBMS: A Relational Database Management System (RDBMS) is used to maintain relational databases.\nSQL: Structured Query Language (SQL) was developed to interact (query and update) with relational databases.\nAdoption: The relational model became dominant in the 1980s, with systems like Oracle, IBM DB2, and Microsoft SQL Server emerging. Nowadays, open-source systems like MySQL are used by big companies to handle their relational data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Rise of NoSQL: 2000s",
    "text": "The Rise of NoSQL: 2000s\n\nLimitations of RDBMS: Traditional relational databases struggled with the scale and complexity of modern web applications.\n\nExample: A social media platform with millions of users posting, commenting, and liking content simultaneously.\nLimitation: RDBMS typically scale vertically (adding more power to a single server), which becomes increasingly expensive and challenging as the database grows. In contrast, NoSQL databases like Cassandra or MongoDB are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nNoSQL Databases: Emerged to address these challenges. They offer flexibility, scalability, and performance improvements. They are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nTypes: Document (e.g., MongoDB), Key-Value (e.g., Redis), Column-Family (e.g., Cassandra), and Graph (e.g., Neo4j).\nUse Cases: Ideal for big data, handling unstructured data, real-time web applications, and distributed systems."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Modern Database Trends",
    "text": "Modern Database Trends\n\n\nNewSQL: Combines the scalability of NoSQL with the Atomicity, Consistency, Isolation, and Durability (ACID) guarantees of traditional relational databases (e.g., Google Spanner).\nCloud Databases: The adoption of cloud computing has led to the rise of managed database services (e.g., Amazon RDS, Google Cloud SQL).\nData Lakes: A storage repository that holds vast amounts of raw data in its native format (e.g., AWS S3, Azure Data Lake)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA relational database links data tables through pre-defined and shared fields in various data tables, establishing relationships.\nThis permits more efficient organization and utilization of data across multiple tables.\nMoreover, a relational database serves as a potent tool for handling extensive data volumes and managing complex data structures."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 1: Flights from a Specific Carrier\n\n\nSELECT flights.year, flights.month, flights.day, \n       flights.flight, airlines.names AS airline_name\nFROM flights\nJOIN airlines ON flights.carrier = airlines.carrier\nWHERE airlines.names = 'American Airlines';  \n-- Replace 'American Airlines' with the desired carrier name"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 2: Weather Conditions at the Time of a Specific Flight\n\n\nSELECT flights.flight, flights.origin, \n        flights.dest, weather.*\nFROM flights\nJOIN weather \nON flights.year = weather.year AND\n   flights.month = weather.month AND\n   flights.day = weather.day AND\n   flights.hour = weather.hour AND\n   flights.origin = weather.origin\nWHERE flights.flight = 'AA123';  \n-- Replace 'AA123' with the desired flight number"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 3: Count of Flights Per Airport\n\n\n\nSELECT airports.faa, \n        COUNT(flights.flight) AS flight_count\nFROM flights\nJOIN airports ON flights.origin = airports.faa\nGROUP BY airports.faa\nORDER BY flight_count DESC;"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse",
    "text": "Data Warehouse\n\n\nA Data Warehouse is a large and comprehensive storage system that consolidates data from various sources, including relational databases, into a centralized repository, much like a university campus that encompasses buildings of various functions.\n\nThe primary purpose of a data warehouse is to facilitate data storage, reporting, and analysis for business intelligence and decision-making purposes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nThe primary goal of an Information Management System (IMS) is to ensure that accurate, timely, and relevant information is generated and available to the right people at the right time, enabling efficient and informed decision-making processes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nKey Components: Data sources, ETL (Extract, Transform, Load), Data Warehouse, OLAP Engine, and Analytic Reporting."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nFinance Data: Information related to financial transactions, budgeting, and accounting.\nCRM Data: Customer Relationship Management data, including customer interactions, sales, and service records.\nOperations Data: Data concerning the day-to-day operations of a business, such as supply chain, inventory, and production.\nMore Data: Any additional data sources that contribute to the organization’s information ecosystem."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "ETL Process (Extract, Transform, Load)",
    "text": "ETL Process (Extract, Transform, Load)\n\n\nExtract: Data is collected from various sources, such as finance systems, CRM systems, and operations databases.\nTransform: The extracted data is cleaned, aggregated, and formatted to fit the data warehouse schema.\nLoad: The transformed data is loaded into the Data Warehouse, where it is stored and made available for analysis."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse and OLAP Engine",
    "text": "Data Warehouse and OLAP Engine\n\n\nData Warehouse: A centralized repository that stores integrated data from multiple sources, optimized for query and analysis.\nOLAP Engine (Online Analytical Processing): Tools that allow for complex analytical queries and multi-dimensional data analysis.\n\nExample: Analyzing sales trends over time, across different regions, or by product categories."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Analytic Reporting and Advanced Analytics",
    "text": "Analytic Reporting and Advanced Analytics\n\n\nAnalytic Reporting Engine: Produces reports and dashboards for users to visualize and understand the data.\n\nAd Hoc Reporting: Enables users to create custom reports on-demand.\nDashboards: Provides a visual summary of key performance indicators (KPIs) and metrics.\n\nAdvanced Analytics: Includes data mining, predictive modeling, and other sophisticated analytical techniques to uncover hidden patterns and insights."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Users and Decision-Making",
    "text": "Users and Decision-Making\n\n\nUsers: Business analysts, managers, and executives who use the IMS to make informed decisions.\nOutcome: The IMS enables data-driven decision-making, improving efficiency, reducing risks, and enhancing overall business performance."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Meta Data and Data Dictionary",
    "text": "Meta Data and Data Dictionary\n\n\n\nMetadata is essentially information about structured data. It can include details like the date and time a database or file was created, who created it, and what types of information it contains.\nA data dictionary is a more specific type of metadata that describes the structure, content, and format of a dataset. It’s like a guidebook and a codebook that provides a comprehensive list of all the variables or columns in a dataset, along with their definitions, data types, and other attributes.\n\nWithout it, you might get lost in a sea of information and struggle to make sense of it all."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "mtcars",
    "text": "mtcars\n\n\n\nThe mtcars data file provides information on various features of different brands of cars, including their engine size, horsepower, and fuel efficiency. The dataset is structured as a table, where each row represents a different car, and each column represents a different variable or feature of the cars."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Dictionary for mtcars",
    "text": "Data Dictionary for mtcars\n\n\n?mtcars"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Learning Path",
    "text": "Learning Path\nThere are plenty of college courses to choose (course titles may vary by schools):\n\nDatabase Management Systems: This course focuses on the design, implementation, and management of databases, teaching students how to organize and manage data effectively.\nInformation Security and Privacy: This course covers the principles and practices of securing information and ensuring data privacy, preparing students to handle data security challenges.\nData Governance and Management: This course explores the governance and management of data assets, including data quality, data integration, and data lifecycle management.\nInformation Systems Analysis and Design: This course teaches students how to analyze business requirements and design information systems to meet organizational needs."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nStructured Data: Highly organized and formatted data that is easily searchable (e.g., tables with rows and columns).\nDatabases: Used to store and manage structured data efficiently.\n\nTypes: Include relational databases (RDBMS) like MySQL and NoSQL databases like MongoDB.\nFunctionality: Provides tools for querying, updating, and managing large datasets.\n\nRelational Databases: Organizes data into tables that can be linked by shared keys.\n\nBenefits: Ensures data integrity and supports complex queries and transactions.\nKey Components: Tables, primary and foreign keys, SQL for data manipulation.\n\n\n\n\n\n\nNon-Relational Databases: NoSQL databases designed for unstructured data and scalability.\n- Advantages: Handle large-scale data across distributed systems more effectively than traditional RDBMS.\nMeta Data: Information describing other data, providing context and making it easier to understand.\nData Dictionary: Detailed description of dataset variables, ensuring consistent data usage."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n20%\n\n\nHomework\n30%\n\n\nFinal Project\n40%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (40%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPoster Presentation at the Purdue Undergraduate Research Conference (40%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nIt is your turn! - 5 minutes\n\n\n\nPresent yourself to your left/right colleague and tell her/him what are the current two main passions in your life."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\nMaterials:\n\nBrightspace\nCourse Webpage\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples.\nObjective is more fuzzy:\n\nFind groups of samples that behave similarly.\nFind features that behave similarly.\nFind linear combinations of features with the most variation.\n\nDifficult to know how well we are doing.\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nQuestions we might ask:\n\n\n\n\nIs there a relationship between advertising budget and sales?\nHow strong is the relationship between advertising budget and sales?\nWhich media contribute to sales?\nHow accurately can we predict future sales?\nIs the relationship linear?\nIs there synergy among the advertising media?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values (e.g., gender, ethnicity).\nCategorical predictors can be represented using factor variables.\nQualitative variables: Gender, Student (Student Status), Status (Marital Status), Ethnicity."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between males and females, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person is female} \\\\\n0 & \\text{if } i\\text{th person is male}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person is female} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person is male.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for gender model:\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nGender \\(Female\\)\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, contrary to popular wisdom, females don’t generally have a higher credit card balance than males."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the ethnicity variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is Asian} \\\\\n      0 & \\text{if i-th person is not Asian}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is Caucasian} \\\\\n      0 & \\text{if i-th person is not Caucasian}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is Asian} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is Caucasian}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is African American (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(Asian\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(Caucasian\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares Asian to African American and that’s not significant. Likewise, the Caucasian to African-American is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Linear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#overview",
    "href": "lecture_slides/04_classification/04_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Classification\nLinear versus Logistic Regression\nMaking Predictions\nMultinomial Logistic Regression\n\n\n\nDiscriminant Analysis\nLinear Discriminant Analysis when \\(p &gt; 1\\)\nTypes of errors\nOther Forms of Discriminant Analysis\nNaive Bayes\nGeneralized Linear Models\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classification",
    "href": "lecture_slides/04_classification/04_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "href": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression: Probability of Default",
    "text": "Linear versus Logistic Regression: Probability of Default\n\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\nLogistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "href": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\n\n\n\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant Euler’s number.\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\n\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 1: Express \\(1 - p(X)\\)\n\nSince \\(p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\), we can write:\n\\[\n1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\nSimplify:\n\\[\n1 - p(X) = \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 2: Compute the Odds\n\nThe odds are defined as:\n\\[\n\\frac{p(X)}{1 - p(X)}\n\\]\nSubstitute \\(p(X)\\) and \\(1 - p(X)\\):\n\\[\n\\frac{p(X)}{1 - p(X)} =\n\\frac{\\dfrac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}\n{\\dfrac{1}{1 + e^{\\beta_0 + \\beta_1 X}}}\n\\]\nSimplify:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 3: Take the Log of the Odds\n\nTaking the natural logarithm:\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\log\\!\\Bigl(e^{\\beta_0 + \\beta_1 X}\\Bigr)\n\\]\nSimplify using the log property \\(\\log(e^x) = x\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nConclusion\n\nThe final transformation shows that the log-odds (logit) of \\(p(X)\\) is a linear function of \\(X\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a model by maximizing the likelihood function, which measures how likely the observed data is given the parameters.\n\nThe likelihood function is based on the probability distribution of the data. If you assume that the data points are independent, the likelihood function is the product of the probabilities of each observation.\n\nConsidering a data series of observed zeros and ones, and a model for the probabilities involving parameters (e.g., \\(\\beta_0\\) and \\(\\beta_1\\)), for any specific parameter values, we can compute the probability of observing the data.\nSince the observations are assumed to be independent, the joint probability of the observed sequence is the product of the probabilities for each observation. For each “1,” we use the model’s predicted probability, \\(p(x_i)\\), and for each “0,” we use \\(1 - p(x_i)\\).\nThe goal of MLE is to find the parameter values that maximize this joint probability, as they make the observed data most likely to have occurred."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSuppose you are flipping a coin, and you observe 5 heads out of 10 flips. The coin’s bias (the probability of heads) is \\(p\\), and you want to estimate \\(p\\).\nThe probability of observing a single outcome (heads or tails) follows the Bernoulli distribution:\n\\[\nP(\\text{Heads or Tails}) = p^x (1-p)^{1-x}, \\quad \\text{where } x = 1 \\text{ for heads, } x = 0 \\text{ for tails.}\n\\]\nFor 10 independent flips, the likelihood function is:\n\\[\nL(p) = P(\\text{data} \\mid p) = \\prod_{i=1}^{10} p^{x_i}(1-p)^{1-x_i}.\n\\]\nIf there are 5 heads (\\(x=1\\)) and 5 tails (\\(x=0\\)):\n\\[\nL(p) = p^5 (1-p)^5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSimplify with the Log-Likelihood\nSince multiplying probabilities can result in very small numbers, we take the logarithm of the likelihood (log-likelihood). The logarithm simplifies the product into a sum:\n\\[\n\\ell(p) = \\log L(p) = \\log \\left(p^5 (1-p)^5\\right) = 5\\log(p) + 5\\log(1-p).\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nMaximize the Log-Likelihood\nTo find the value of \\(p\\) that maximizes \\(\\ell(p)\\), take the derivative of the log-likelihood with respect to \\(p\\) and set it to zero:\n\\[\n\\frac{\\partial\\ell(p)}{\\partial p} = \\frac{5}{p} - \\frac{5}{1-p} = 0.\n\\]\nSimplify:\n\\[\n\\frac{5}{p} = \\frac{5}{1-p}.\n\\]\nSolve for \\(p\\):\n\\[\n1 - p = p \\quad \\Rightarrow \\quad 1 = 2p \\quad \\Rightarrow \\quad p = 0.5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nTo confirm that \\(p = 0.5\\) is the maximum, you can check the second derivative of the log-likelihood (concavity) or use numerical methods.\nIn our example, \\(p = 0.5\\) makes sense intuitively because the data (5 heads out of 10 flips) suggests the coin is unbiased.\nThe maximum likelihood estimate of \\(p\\) is \\(0.5\\). The MLE method finds the parameter values that make the observed data most likely, given the assumed probability model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nAssumptions:\n\nData \\(x_1, x_2, \\dots, x_n\\) are drawn from a normal distribution with:\n\n\\[\n  f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\n\nAssume \\(\\sigma\\) is known (say, \\(\\sigma = 1\\)) and we want to estimate \\(\\mu\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\nThe likelihood for \\(n\\) independent observations is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x_i - \\mu)^2}{2}}\n\\]\nTaking the natural log:\n\\[\n\\ell(\\mu) = \\log L(\\mu) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2} \\right]\n\\]\nSimplify (since \\(-\\frac{1}{2} \\log(2\\pi)\\) is constant):\n\\[\n\\ell(\\mu) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = -\\sum_{i=1}^n (x_i - \\mu)\n\\] Set this to zero:\n\\[\n\\sum_{i=1}^n (x_i - \\mu) = 0\n\\]\nSolve for \\(\\mu\\):\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nThe MLE for the mean \\(\\mu\\) is simply the sample mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "href": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nMost statistical packages can fit linear logistic regression models by maximum likelihood.\nLogistic Regression Coefficients\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.6513\n0.3612\n-29.5\n&lt; 0.0001\n\n\nbalance\n0.0055\n0.0002\n24.9\n&lt; 0.0001\n\n\n\n\nWhat is our estimated probability of default for someone with a credit card balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a a credit card balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent \\(Yes\\)\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\n\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent Yes\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding",
    "href": "lecture_slides/04_classification/04_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "href": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\text{Softmax}(z_k) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nWe really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example will given later in the course, when we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the Softmax Function?",
    "text": "What is the Softmax Function?\n\n\nThe softmax function is used in multinomial logistic regression to convert raw scores (logits) into probabilities for multiple classes.\n\n\nLogits are the raw, untransformed output of the linear component in logistic regression. For a given class \\(k\\), the logit is defined as:\n\\[\nz_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\n\\]\nWhere:\n\n\\(z_k\\): The logit for class \\(k\\).\n\\(\\beta_{k0}\\): Intercept term.\n\\(\\beta_{kj}\\): Coefficients for predictor \\(x_j\\).\n\n\n\nSoftmax Definition:\nFor \\(K\\) classes and input \\(x\\), the softmax function is defined as:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}}\n\\]\nWhere:\n\n\\(z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\\): The linear score (logit) for class \\(k\\).\n\\(\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp}\\): Coefficients for class \\(k\\).\n\\(e^{z_k}\\): Exponentiated score for class \\(k\\), ensuring all values are positive.\n\n\n\n\nKey Features of the Softmax Function\n\nProbability Distribution: Outputs probabilities that sum to 1 across all \\(K\\) classes. \\(\\text{Pr}(Y = k \\mid X = x) = \\text{Softmax}(z_k)\\).\nNormalization: Normalizes logits by dividing each exponentiated logit by the sum of all exponentiated logits.\nHandles Multiclass Classification: Extends binary logistic regression to \\(K &gt; 2\\) classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "href": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example of Softmax in Action",
    "text": "Example of Softmax in Action\n\n\n\n\nImagine classifying three emergency room conditions: Stroke, Drug Overdose, and Epileptic Seizure.\nSuppose the logits are: \\(z_{\\text{stroke}} = 2.5, \\quad z_{\\text{drug overdose}} = 1.0, \\quad z_{\\text{epileptic seizure}} = 0.5\\)\nThe probabilities are:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{e^{2.5} + e^{1.0} + e^{0.5}}\n\\]\n\nStep 1: Exponentiate the Logits\n\\(e^{z_{\\text{stroke}}} = e^{2.5} \\approx 12.182\\)\n\\(e^{z_{\\text{drug overdose}}} = e^{1.0} \\approx 2.718\\)\n\\(e^{z_{\\text{epileptic seizure}}} = e^{0.5} \\approx 1.649\\)\n\n\nStep 2: Compute the Denominator\n\\(\\sum_{l=1}^K e^{z_l} = e^{2.5} + e^{1.0} + e^{0.5}\\)\n\\(\\sum_{l=1}^K e^{z_l} \\approx 12.182 + 2.718 + 1.649 = 16.549\\)\n\n\n\nStep 3: Calculate the Probabilities\n\\(\\text{Pr}(\\text{stroke}) = \\frac{e^{z_{\\text{stroke}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{12.182}{16.549} \\approx 0.7366\\)\n\\(\\text{Pr}(\\text{drug overdose}) = \\frac{e^{z_{\\text{drug overdose}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{2.718}{16.549} \\approx 0.1642\\)\n\\(\\text{Pr}(\\text{epileptic seizure}) = \\frac{e^{z_{\\text{epileptic seizure}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{1.649}{16.549} \\approx 0.0996\\)\nThe output probabilities represent the likelihood of each condition, ensuring:\n\\[\n\\sum_{k=1}^3 \\text{Pr}(Y = k) = 1\n\\]\nWe have:\n\\[\n   0.7366 + 0.1642 + 0.0996 \\approx 1.000\n\\]\n\n\nConclusion\n\nThe softmax function translates raw scores into probabilities, making it essential for multiclass classification.\nIt ensures a probabilistic interpretation while maintaining normalization across all classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions as input for \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes’ Theorem: Explanation",
    "text": "Bayes’ Theorem: Explanation\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A|B)\\): Posterior probability - Probability of event \\(A\\) occurring given that \\(B\\) is true — updated probability after the evidence is considered.\n\\(P(A)\\): Prior probability - Initial probability of event \\(A\\) — the probability before the evidence is considered.\n\\(P(B|A)\\): Likelihood - Probability of observing event \\(B\\) given that \\(A\\) is true.\n\\(P(B)\\): Marginal probability - Total probability of the evidence, event \\(B\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "href": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Understanding Conditional Probability",
    "text": "Understanding Conditional Probability\nConditional probability is the probability of an event occurring given that another event has already occurred.\nDefinition:\n\\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nis the probability of event \\(A\\) occurring given that \\(B\\) is true.\n\nInterpretation: How likely is \\(A\\) if we know that \\(B\\) happens?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Joint Probability?",
    "text": "What is Joint Probability?\nJoint probability refers to the probability of two events occurring together.\nDefinition: \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur.\n\nConnection to Conditional Probability:\n\\[\n  P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n  P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\nThis formula is crucial for understanding Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "href": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Symmetry in Joint Events",
    "text": "Symmetry in Joint Events\nJoint probability is symmetric, meaning:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, we can also express it as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\nThis symmetry is the key to deriving Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "href": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deriving Bayes’ Theorem",
    "text": "Deriving Bayes’ Theorem\n\n\nGiven that the definition of Conditional Probability is:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\nUsing the Definition of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n   P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\n\n\n\nSymmetry of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(B \\cap A)\n\\]\n\n\nThus, we can express the joint probability as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\n\nThe Bayes’ Theorem!\n\nSubstitute this back into the conditional probability definition:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "href": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Bayes’ Theorem Matters?",
    "text": "Why Bayes’ Theorem Matters?\n\n\nBayes’ Theorem is a foundational principle in probability theory and statistics, enabling:\n\nIncorporation of Prior Knowledge:\nIt allows for the integration of prior knowledge or beliefs when making statistical inferences.\nBeliefs Update:\nIt provides a systematic way to update the probability estimates as new evidence or data becomes available.\nProbabilistic Thinking:\nEncourages a probabilistic approach to decision-making, quantifying uncertainty, and reasoning under uncertainty.\nVersatility in Applications:\nFrom medical diagnosis to spam filtering, Bayes’ Theorem is pivotal in areas requiring probabilistic assessment.\n\nBayes’ Theorem is a paradigm that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\nLeft-hand plot: single variable X and \\(\\pi_k f_k(x)\\) in the vertical axis for both classes \\(k\\) equals 1 and \\(k\\) equals 2. In this case the the pies are the same for both, so anything to the left of zero we classify as as green and anything to the right we classify as as purple.\nRight-hand plot: here we have different priors. The probability of \\(k = 2\\) is 0.7 and and of of \\(k= 1\\) is 0.3. The decision boundary moved slightly to the left. On the right, we favor the pink class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\n\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify one observation at the value \\(X = x\\) into a class, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nLeft-Panel: Synthetic population data with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule.\nRight-Panel: histograms of the sample. We see that the estimation provided a decision boundary (black solid line) pretty close to the correct one, the one of the population."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "href": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\n\nThe prior is the number in each class divided by the total number:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nThe means in each class is the sample mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\nWe assume that the variance is the same in each of the classes and so we assume a pooled variance estimate:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nGaussian density in two Dimensions, two variables \\(x_1\\) and \\(x_2\\). On the Left-panel, we have a bell function and this is the case when the two variables are uncorrelated. On the Right-panel, there is correlation between the two predictors and it is like a stretched bell.\nDensity:\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\] where \\(\\Sigma\\) is the covariance matrix."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\n\n\nThe covariance matrix is a square matrix that summarizes the covariance (a measure of how much two random variables vary together) between multiple variables in a dataset.\nDefinition:\nFor a random vector \\(X = [X_1, X_2, \\dots, X_p]^\\top\\) with \\(p\\) variables, the covariance matrix \\(\\Sigma\\) is defined as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n\\end{bmatrix}\n\\]\n\nKey Properties:\n\n\\(\\text{Var}(X_i)\\): Variance of variable \\(X_i\\).\n\\(\\text{Cov}(X_i, X_j)\\): Covariance between variables \\(X_i\\) and \\(X_j\\).\n\\(\\Sigma\\) is symmetric: \\(\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)\\).\nDiagonal elements represent variances, and off-diagonal elements represent covariances."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nDiscriminant function: after simplifying the density function we can find\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\]\nNote that it is a linear function where the first component, \\(x^T \\Sigma^{-1} \\mu_k\\), has the \\(x\\) variable multiplied by a coefficient vector and, the second component, \\(\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\), is a constant."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nThe Discriminant function can be written as\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\]\na linear function. That is a function for class \\(k\\), where \\(c_{k0}\\) represents the constant we find in the second component of the Discriminant function and \\(c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\) come from the first component of the Discriminant function. We compute \\(\\delta_k(x)\\) for each of the classes and then you classify to the class for which it is largest."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\n\nLeft-panel: The circle presents the countor of the density of a particular level of probability for the blue, green, and the orange class. Here \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\). The dashed lines are known as the Bayes decision boundaries. They are the “True” decision boundaries, were they known, they would yield the fewest misclassification errors, among all possible classifiers.\nRight-panel: We compute the mean for \\(x_1\\) and \\(x_2\\) for the each blue, green, and orange class. After plugging them into the formula, instead of getting the the dotted lines we get the solid black lines."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Iris Data",
    "text": "Example: Fisher’s Iris Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 variables\n3 species\n50 samples/class\n\n🟦 Setosa\n🟧 Versicolor\n🟩 Virginica\n\nLDA classifies all but 3 of the 150 training samples correctly."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Discriminant Plot",
    "text": "Example: Fisher’s Discriminant Plot\n\n\nDiscriminant variables 1 and 2 are linear combinations of the original variables.\nLDA classifies points based on their proximity to centroids in discriminant space.\nThe centroids lie in a subspace of the multi-dimensional space (e.g., a plane within 4D space).\nFor \\(K\\) classes:\n\nLDA can be visualized in \\(K - 1\\)-dimensional space.\nFor \\(K &gt; 3\\), the “best” 2D plane can be chosen for visualization."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\n\n\nOnce we have estimates of the Discriminat Functions, \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "href": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Default Status\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\n\n\n\n\\(\\frac{23 + 252}{10000}\\) errors — a 2.75% misclassification rate!\n\n\nSome caveats:\n\nThis is training error, and we may be overfitting.\nIf we classified to the prior, the proportion of cases in the classes (e.g. always assuming the class No default). We would make \\(\\frac{333}{10000}\\) errors, or only 3.33%. This is what we call the null rate.\nWe can break the errors into different kinds: of the true No’s, we make \\(\\frac{23}{9667} = 0.2\\%\\) errors; of the true Yes’s, we make \\(\\frac{252}{333} = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "href": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "href": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "href": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC plot displays both simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\n\n\n\n\n\n\n\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\nIn the Left-plot we see a case when the true boundary should be linear. In the Right-plot, covariances were different in the true data. It is possible to see that the bayes decision boundary is curved and the quadratic discriminant analysis is also curved whereas the linear discriminant analysis gives a different boundary.\nWhether each class has the same or different covariance matrices significantly impacts how boundaries between the classes are defined. The covariance matrix describes the spread or variability of data points within each class and how the features in that class relate to each other.\n\nKey Insight: If \\(\\Sigma_k\\) are different for each class, the quadratic terms matter significantly.\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nExample: Suppose we are classifying plants based on two features (e.g., height and leaf width). If one type of plant has a tall and narrow spread of data, while another type has a short and wide spread, QDA can handle these differences and draw curved boundaries to separate the groups."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "href": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assess the Covariance Matrices",
    "text": "Assess the Covariance Matrices\n\nLDA assumes the covariance matrices of all classes are the same, while QDA allows each class to have its own. To determine which assumption is better:\n\nHypothesys test: we can perform a Test for Equality of Covariance Matrices (e.g. Box’s M Test). If the covariance matrices are similar (test is not significant): LDA is appropriate. If the covariance matrices differ (test is significant): QDA may be better.\nVisual Inspection: Plot the data in two dimensions (e.g., using scatterplots). Check if the spread, shape, or orientation of data points differs significantly between classes. If they are similar, LDA might work well. If they are visibly different, QDA is likely better.\nCompare Model Performance: run both models and choose the model that performs better on unseen data (test set).\nConsider the Number of Features and Data Size: LDA performs well with smaller datasets because it estimates a single covariance matrix across all classes (fewer parameters). QDA requires a larger dataset because it estimates a separate covariance matrix for each class (more parameters).\nDomain Knowledge: Use your understanding of the data to decide."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\n\nFor a two-class problem, one can show that for LDA:\n\\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\nif we take the log odds, \\(\\log \\left( \\frac{p_1(x)}{1 - p_1(x)}\\right)\\), which is the log of the probability for class 1 versus the probability for class two, we endup with a linear function of \\(x\\), \\(c_0 + c_1 x_1 + \\dots + c_p x_p\\). So it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\). In Machine Learning, it is known as discriminative learning.\nLDA uses the full likelihood based on the joint distributions of \\(x's\\) and \\(y's\\), \\(\\text{Pr}(X, Y)\\), whereas logistic regression was only using the distribution of \\(y's\\). It is known as generative learning.\nDespite these differences, in practice, the results are often very similar.\n\nLogistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes assumes each \\(\\Sigma_k\\) is diagonal:\n\\[\n\\begin{aligned}\n\\delta_k(x) &\\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\end{aligned}\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\nKey Point: Despite strong assumptions, naive Bayes often produces good classification results.\n\nExplanation:\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_{kj}(x_j)\\): Density function for feature \\(j\\) in class \\(k\\).\n\\(\\mu_{kj}\\): Mean of feature \\(j\\) in class \\(k\\).\n\\(\\sigma_{kj}^2\\): Variance of feature \\(j\\) in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diagonal Covariance Matrix",
    "text": "Diagonal Covariance Matrix\n\n\nA diagonal covariance matrix is a special case of the covariance matrix where all off-diagonal elements are zero. This implies that the variables are uncorrelated.\nGeneral Form:\nFor \\(p\\) variables, a diagonal covariance matrix \\(\\Sigma\\) is represented as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\nProperties:\n\nDiagonal Elements (\\(\\sigma_i^2\\)): Represent the variance of each variable \\(X_i\\).\nOff-Diagonal Elements: All equal to zero (\\(\\text{Cov}(X_i, X_j) = 0\\) for \\(i \\neq j\\)), indicating no linear relationship between variables.\nA diagonal covariance matrix assumes independence between variables. Each variable varies independently without influencing the others.\nCommonly used in simpler models, such as Naive Bayes, where independence is assumed."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nUseful if some classes are well separated. A situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example: \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\\) We estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis toy example demonstrates the working of the Naïve Bayes classifier for two classes (\\(k = 1\\) and \\(k = 2\\)) and three features (\\(X_1, X_2, X_3\\)). The goal is to compute the posterior probabilities \\(\\Pr(Y = 1 \\mid X = x^*)\\) and \\(\\Pr(Y = 2 \\mid X = x^*)\\) for a given observation \\(x^* = (0.4, 1.5, 1)\\).\n\nThe prior probabilities for each class are:\n\\(\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\\)\n\n\nFor each feature (\\(X_1, X_2, X_3\\)), we estimate the class-conditional density functions:\n\n\\(\\hat{f}_{11}, \\hat{f}_{12}, \\hat{f}_{13}\\): Densities for \\(k = 1\\) (class 1).\n\n\\(\\hat{f}_{11}(0.4) = 0.368 \\\\\\)\n\\(\\hat{f}_{12}(1.5) = 0.484 \\\\\\)\n\\(\\hat{f}_{13}(1) = 0.226 \\\\\\)\n\n\\(\\hat{f}_{21}, \\hat{f}_{22}, \\hat{f}_{23}\\): Densities for \\(k = 2\\) (class 2).\n\n\\(\\hat{f}_{21}(0.4) = 0.030 \\\\\\)\n\\(\\hat{f}_{22}(1.5) = 0.130 \\\\\\)\n\\(\\hat{f}_{23}(1) = 0.616 \\\\\\)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\nCompute Class-Conditional Likelihoods for each class \\(k\\), the likelihood is computed as the product of the conditional densities for each feature:\n\n\\[\n   \\hat{f}_k(x^*) = \\prod_{j=1}^3 \\hat{f}_{kj}(x_j^*)\n\\]\n\nFor \\(k = 1\\):\n\n\\[\n     \\hat{f}_{11}(0.4) = 0.368, \\quad \\hat{f}_{12}(1.5) = 0.484, \\quad \\hat{f}_{13}(1) = 0.226\n\\]\n\\[\n     \\hat{f}_1(x^*) = 0.368 \\times 0.484 \\times 0.226 \\approx 0.0402\n\\]\n\nFor \\(k = 2\\):\n\n\\[\n     \\hat{f}_{21}(0.4) = 0.030, \\quad \\hat{f}_{22}(1.5) = 0.130, \\quad \\hat{f}_{23}(1) = 0.616\n\\]\n\\[\n     \\hat{f}_2(x^*) = 0.030 \\times 0.130 \\times 0.616 \\approx 0.0024\n\\]\n\n\n\nCompute Posterior Probabilities using Bayes’ theorem:\n\n\\[\n   \\Pr(Y = k \\mid X = x^*) = \\frac{\\hat{\\pi}_k \\hat{f}_k(x^*)}{\\sum_{k=1}^2 \\hat{\\pi}_k \\hat{f}_k(x^*)}\n\\]\n\nFor \\(k = 1\\): \\[\n\\Pr(Y = 1 \\mid X = x^*) = \\frac{0.5 \\times 0.0402}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.944\n\\]\nFor \\(k = 2\\): \\[\n\\Pr(Y = 2 \\mid X = x^*) = \\frac{0.5 \\times 0.0024}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.056\n\\]\n\n\n\nKey Takeaways:\n\nNaïve Bayes Assumption: The assumption of feature independence simplifies computation by allowing the class-conditional densities to be computed separately for each feature.\nPosterior Probabilities: The posterior probability combines the prior (\\(\\pi_k\\)) and the likelihood (\\(\\hat{f}_k(x^*)\\)).\nClassification: The observation \\(x^*\\) is classified as the class with the highest posterior probability (\\(Y = 1\\))."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\n\nNaïve Bayes classifier can be understood as a special case of a GAM.\n\\[\n\\begin{aligned}\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n&= \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right) \\\\\n&= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\end{aligned}\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model is a Generalized Additive Model (GAM):\n\nThe log-odds are expressed as a sum of additive terms.\n\\(a_k\\): Represents prior influence.\n\\(g_{kj}(x_j)\\): Represents feature contributions."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nLog-Odds of Posterior Probabilities\nThe Naïve Bayes classifier starts with the log-odds of the posterior probabilities:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n\\]\nThis is the log of the ratio of the probabilities of class \\(k\\) and a reference class \\(K\\), given the feature vector \\(X = x\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nBayes’ Theorem\nUsing Bayes’ theorem, the posterior probabilities can be expressed as:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_k(x)\\): Class-conditional density for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nNaïve Bayes Assumption\nThe Naïve Bayes assumption states that features are conditionally independent given the class:\n\\[\nf_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n\\]\nSubstituting this into the equation:\n\\[\n\\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) = \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nSeparate the Terms\nThe terms can now be separated:\n\\[\n\\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\n\\(\\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\): Influence of prior probabilities.\n\\(\\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\): Contribution from each feature."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nAdditive Form\nDefine:\n\\[\na_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right), \\quad g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\nThe equation becomes:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = a_k + \\sum_{j=1}^p g_{kj}(x_j)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\n\n\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "href": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Mean/Variance Relationship",
    "text": "Example: Mean/Variance Relationship\n\n\nLeft plot: we see that the variance mostly increases with the mean.\n10% of a linear model predictions are negative! (not shown here.). However, we know that the response variable, bikers, is always positive.\nTaking log(bikers) alleviates this, but is not a good solution. It has its own problems: e.g. predictions are on the wrong scale, and some counts are zero!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "href": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nPoisson distribution is useful for modeling counts:\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\nMean/variance relationship: \\(\\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\\) i.e., there is a mean/variance dependence. When the mean is higher, the variance is higher.\nModel with Covariates:\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nOr equivalently:\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAutomatic positivity: The model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Poisson Regression on Bikeshare Data",
    "text": "Example: Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote: in this case, the variance is somewhat larger than the mean — a situation known as overdispersion. As a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\n\nWe have covered three GLMs: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\), \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include Gamma, Negative-binomial, Inverse Gaussian, and more."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#summary-1",
    "href": "lecture_slides/04_classification/04_classification.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside [0, 1].\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nMotivation\nTraining Error versus Test Error\nValidation-Set Approach\nCross-Validation\nCross-Validation for Classification Problems\n\n\n\nBootstrap\nMore on Bootstrap\nCan the Bootstrap Estimate Prediction Error?\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXXX",
    "text": "XXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-validation and the Bootstrap",
    "text": "Cross-validation and the Bootstrap\n\nIn this section we discuss two resampling methods: cross-validation and the bootstrap.\nThese methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nIn contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training- versus Test-Set Performance",
    "text": "Training- versus Test-Set Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHorizontal Axis: Represents model complexity (low to high).\n\nLow complexity: Simpler models with fewer parameters (e.g., fitting a straight line or using a few features).\nHigh complexity: More complex models with many parameters (e.g., higher-degree polynomials or many features).\n\nVertical Axis: Represents prediction error.\n\nLower values indicate better predictive performance.\n\n\n\n\nTraining Error (Blue Curve):\n\nStarts high at low complexity because simple models underfit the training data.\nDecreases steadily as the model becomes more complex, fitting the training data better.\nContinues to decline even as the model becomes overly complex.\n\nTest Error (Red Curve):\n\nStarts high at low complexity due to underfitting (failure to generalize).\nDecreases as complexity increases and the model starts capturing relevant patterns.\nReaches a minimum at the optimal complexity (sweet spot).\nIncreases again at high complexity due to overfitting (model captures noise instead of general patterns).\n\n\n\nKey Concepts\nBias-Variance Tradeoff:\n\nHigh Bias (Left Side): Simple models fail to capture the true structure of the data.\nHigh Variance (Right Side): Complex models become overly tailored to the training data and fail to generalize.\n\nOptimal Complexity:\n\nLocated where the test error is minimized.\nBalances bias and variance for the best generalization performance.\n\nThe Goal is to select a model complexity that minimizes test error to ensure good predictive performance on unseen data."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Prediction-Error Estimates",
    "text": "More on Prediction-Error Estimates\n\nBest solution: test the model with a large test set.\n\nHowever, it is not very often available.\n\nIn the absence of a large test set, some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate.\n\nThese include the Cp statistic, AIC, and BIC.\n\nIn this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held-out observations."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error.\n\nThis is typically assessed using the Mean Squared Error (MSE) in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\n\nA random splitting of the original dataset into two halves (two-fold validation):\n\nLeft part is the training set\nRight part is the validation set"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\nA random splitting into two halves:\nLeft part is the training set, and the right part is the validation set."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Automobile Data",
    "text": "Example: Automobile Data\n\n\n\nWant to compare linear vs higher-order polynomial terms in a linear regression.\nWe randomly split the 392 observations into two sets:\n\nA training set containing 196 of the data points.\nA validation set containing the remaining 196 observations.\n\n\n\n\n\n\n\n\n\n\n\nLeft panel shows single split; right panel shows multiple splits."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Drawbacks of Validation Set Approach",
    "text": "Drawbacks of Validation Set Approach\n\nThe validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nIn the validation approach, only a subset of the observations — those that are included in the training set rather than in the validation set — are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. Why?\n\nHaving more data generally leads to lower error because it provides more information for training the model.\nFor example, training on 200 observations is typically preferable to 100 observations, as larger datasets improve accuracy.\nHowever, when the training set is reduced (e.g., during validation), error estimates can be higher since smaller datasets may fail to capture all patterns in the data.\nThis limitation highlights the drawbacks of simple validation.\nCross-validation addresses this issue by efficiently using the data to produce more accurate and reliable error estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 5\\) here)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Details",
    "text": "The Details\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSetting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Nice Special Case!",
    "text": "A Nice Special Case!\nWith least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\\[\n  \\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2,\n\\]\nwhere \\(\\hat{y}_i\\) is the \\(i\\)-th fitted value from the original least-squares fit, and \\(h_i\\) is the leverage (diagonal of the “hat” matrix; see book for details). This is like the ordinary MSE, except the \\(i\\)-th residual is divided by \\(1 - h_i\\).\n\nLOOCV is sometimes useful, but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated, and hence their average can have high variance.\nA better choice is \\(K = 5\\) or \\(K = 10\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Auto Data Revisited",
    "text": "Auto Data Revisited"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "True and Estimated Test MSE for the Simulated Data",
    "text": "True and Estimated Test MSE for the Simulated Data\n\n\n\n\n\n\n\n\n\n\n\nThe plot presents the cross-validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in Figures 2.9–2.11 of Chapter 2 of the book.\nThe true test MSE is displayed in blue.\nThe black dashed and orange solid lines respectively show the estimated LOOCV and 10-fold CV estimates.\nIn all three plots, the two cross-validation estimates are very similar.\nRight-hand panel: the true test MSE and the cross-validation curves are almost identical.\nCenter panel: the two sets of curves are similar at the lower degrees of flexibility, while the CV curves overestimate the test set MSE for higher degrees of flexibility.\nLeft-hand panel: the CV curves have the correct general shape, but they underestimate the true test MSE."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Issues with Cross-Validation",
    "text": "Other Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation: Right and Wrong",
    "text": "Cross-Validation: Right and Wrong\n\nConsider a simple classifier applied to some two-class data:\n\nStarting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with the class labels.\nWe then apply a classifier such as logistic regression, using only these 100 predictors.\n\n\nHow do we estimate the test set performance of this classifier?\nCan we apply cross-validation in step 2, forgetting about step 1?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NO!",
    "text": "NO!\n\nThis would ignore the fact that in Step 1, the procedure has already seen the labels of the training data, and made use of them. This is a form of training and must be included in the validation process.\nIt is easy to simulate realistic data with the class labels independent of the outcome, so that true test error = 50%, but the CV error estimate that ignores Step 1 is zero! Try to do this yourself.\nWe have seen this error made in many high-profile genomics papers."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Wrong and Right Way",
    "text": "The Wrong and Right Way\n\nWrong: Apply cross-validation in step 2.\nRight: Apply cross-validation to steps 1 and 2."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Wrong Way",
    "text": "Wrong Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Right Way",
    "text": "Right Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Where Does the Name Come From?",
    "text": "Where Does the Name Come From?\n\nThe use of the term bootstrap derives from the phrase to pull oneself up by one’s bootstraps, widely thought to be based on one of the eighteenth-century The Surprising Adventures of Baron Munchausen by Rudolph Erich Raspe:\n\nThe Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.\n\nIt is not the same as the term bootstrap used in computer science, meaning to “boot” a computer from a set of core instructions, though the derivation is similar."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Simple Example",
    "text": "A Simple Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha) Y).\\)\nOne can show that the value that minimizes the risk is given by:\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\] where \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nBut the values of \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), and \\(\\sigma_{XY}\\) are unknown.\nWe can compute estimates for these quantities, \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\), and \\(\\hat{\\sigma}_{XY}\\), using a data set that contains measurements for \\(X\\) and \\(Y\\).\nWe can then estimate the value of \\(\\alpha\\) that minimizes the variance of our investment using:\n\n\n\\[\n  \\hat{\\alpha} = \\frac{\\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 - 2\\hat{\\sigma}_{XY}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\n\n\n\n\n\n\n\n\nEach panel displays 100 simulated returns for investments X and Y. From left to right and top to bottom, the resulting estimates for \\(\\alpha\\), the fraction to minimize the total risk, are 0.576, 0.532, 0.657, and 0.651."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\nTo estimate the standard deviation of \\(\\hat{\\alpha}\\), we repeated the process of simulating 100 paired observations of \\(X\\) and \\(Y\\), and estimating \\(\\alpha\\) 1,000 times.\nWe thereby obtained 1,000 estimates for \\(\\alpha\\), which we can call \\(\\hat{\\alpha}_1, \\hat{\\alpha}_2, \\ldots, \\hat{\\alpha}_{1000}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left-hand panel of the Figure displays a histogram of the resulting estimates.\nFor these simulations, the parameters were set to \\(\\sigma_X^2 = 1, \\, \\sigma_Y^2 = 1.25, \\, \\sigma_{XY} = 0.5,\\) and so we know that the true value of \\(\\alpha\\) is 0.6 (indicated by the red line)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\nThe mean over all 1,000 estimates for \\(\\alpha\\) is:\n\\[\n  \\bar{\\alpha} = \\frac{1}{1000} \\sum_{r=1}^{1000} \\hat{\\alpha}_r = 0.5996,\n\\]\nvery close to \\(\\alpha = 0.6\\), and the standard deviation of the estimates is:\n\n\\[\n  \\sqrt{\\frac{1}{1000 - 1} \\sum_{r=1}^{1000} (\\hat{\\alpha}_r - \\bar{\\alpha})^2} = 0.083.\n\\]\n\nThis gives us a very good idea of the accuracy of \\(\\hat{\\alpha}\\): \\(\\text{SE}(\\hat{\\alpha}) \\approx 0.083\\).\nSo roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately 0.08, on average."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nPre-validated\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now Back to the Real World",
    "text": "Now Back to the Real World\n\nThe procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.\nHowever, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.\nRather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.\nEach of these “bootstrap data sets” is created by sampling with replacement, and is the same size as our original dataset. As a result, some observations may appear more than once in a given bootstrap data set and some not at all."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example with Just 3 Observations",
    "text": "Example with Just 3 Observations\n\n\nA graphical illustration of the bootstrap approach on a small sample containing \\(n = 3\\) observations.\nEach bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set.\nEach bootstrap data set is used to obtain an estimate of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bootstrap Standard Error",
    "text": "Bootstrap Standard Error\n\nDenoting the first bootstrap data set by \\(Z^{*1}\\), we use \\(Z^{*1}\\) to produce a new bootstrap estimate for \\(\\alpha\\), which we call \\(\\hat{\\alpha}^{*1}\\).\nThis procedure is repeated \\(B\\) times for some large value of \\(B\\) (say 100 or 1000), in order to produce \\(B\\) different bootstrap data sets, \\(Z^{*1}, Z^{*2}, \\ldots, Z^{*B}\\), and \\(B\\) corresponding \\(\\alpha\\) estimates, \\(\\hat{\\alpha}^{*1}, \\hat{\\alpha}^{*2}, \\ldots, \\hat{\\alpha}^{*B}\\).\nWe estimate the standard error of these bootstrap estimates using the formula:\n\n\n\\[\nSE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B - 1} \\sum_{r=1}^B (\\hat{\\alpha}^{*r} - \\bar{\\alpha}^{*})^2}.\n\\]\n\nThis serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. See center and right panels of Figure on slide 29. Bootstrap results are in blue.\nFor this example \\(SE_B(\\hat{\\alpha}) = 0.087\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A General Picture for the Bootstrap",
    "text": "A General Picture for the Bootstrap\n\n\n\n\n\nReal World\n\nPopulation \\(P\\)\n\nWe imagine there is a true, unknown population (or data‐generating process).\n\nIn practice, we typically do not have direct access to all of \\(P\\).\n\nRandom Sampling\n\nWe draw a finite sample \\(Z = (z_1, z_2, \\dots, z_n)\\) from the population \\(P\\).\n\nThis sample \\(Z\\) is our observed dataset (often called the “training data” in applied work).\n\nEstimate \\(f(Z)\\)\n\nFrom this observed data \\(Z\\), we compute a statistic or estimate, denoted \\(f(Z)\\).\n\nExamples might include a mean, a regression coefficient, or (in the investment example) an optimal allocation parameter \\(\\alpha\\).\n\n\nIn short, the Real World side shows how our single dataset \\(Z\\) arrives by randomly sampling from the true population \\(P\\).\n\nBootstrap World\n\nEstimated Population \\(\\hat{P}\\)\n\nBecause we usually cannot sample repeatedly from the real population \\(P\\), the bootstrap creates a stand‐in population \\(\\hat{P}\\). We ‘replace’ the population by our sample.\n\\(\\hat{P}\\) is the empirical distribution function of the observed data \\(Z\\). Informally, it assigns probability \\(\\tfrac{1}{n}\\) to each observed point in \\(Z\\).\n\nRandom Sampling from \\(\\hat{P}\\)\n\nTo mimic drawing new data from the real population, we instead draw (with replacement) from \\(\\hat{P}\\).\n\nThis produces a bootstrap dataset \\(Z^* = (z_1^*, z_2^*, \\dots, z_n^*)\\). Each \\(z_i^*\\) is sampled (with replacement) from among the original observed points \\(\\{z_1, \\dots, z_n\\}\\).\n\nBootstrap Estimate \\(f(Z^*)\\)\n\nWe compute the same statistic (or estimator) on each bootstrap sample, giving \\(f(Z^*)\\).\n\nBy repeating this bootstrap sampling many times, we obtain a distribution of estimates \\(\\{f(Z^*_1), f(Z^*_2), \\dots\\}\\). This approximates how \\(f(Z)\\) would vary if we could repeatedly resample from the true population."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap in General",
    "text": "The Bootstrap in General\n\nIn more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.\nFor example, if the data is a time series, we can’t simply sample the observations with replacement (why not?).\n\nThe main reason we typically cannot simply resample individual points with replacement in a time series is that time‐ordered data exhibits serial dependence. That is, adjacent observations (e.g., today’s stock price and yesterday’s stock price) are correlated in ways that we lose if we treat all observations as independent units and shuffle them arbitrarily.\nA simple i.i.d. bootstrap would ignore the natural ordering of the data points (and the correlations it encodes), thereby violating a crucial assumption about the structure of time‐series data.\n\nWe can instead create blocks of consecutive observations and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Uses of the Bootstrap",
    "text": "Other Uses of the Bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the figure on slide 29, the 5% and 95% quantiles of the 1,000 values is (0.43, 0.72).\nThis represents an approximate 90% confidence interval for the true α. How do we interpret this confidence interval?\nThe above interval is called a Bootstrap Percentile confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. Can you prove this?\nThis will cause the bootstrap to seriously underestimate the true prediction error. Why?\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Overlap",
    "text": "Removing the Overlap\n\nCan partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.\nBut the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation",
    "text": "Pre-validation\n\nIn microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.\nComparing them on the same dataset that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.\nPre-validation can be used to make a fairer comparison between the two sets of predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Motivating Example",
    "text": "Motivating Example\nAn example of this problem arose in the paper of van’t Veer et al. Nature (2002). Their microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer. There are 44 cases in the good prognosis group and 34 in the poor prognosis group. A “microarray” predictor was constructed as follows:\n\n70 genes were selected, having the largest absolute correlation with the 78 class labels.\nUsing these 70 genes, a nearest-centroid classifier \\(C(x)\\) was constructed.\nApplying the classifier to the 78 microarrays gave a dichotomous predictor \\(z_i = C(x_i)\\) for each case \\(i\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nPre-validated\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Idea behind Pre-validation",
    "text": "Idea behind Pre-validation\n\nDesigned for comparison of adaptively derived predictors to fixed, pre-defined predictors.\nThe idea is to form a “pre-validated” version of the adaptive predictor: specifically, a “fairer” version that hasn’t “seen” the response \\(y\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation Process",
    "text": "Pre-validation Process"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation in Detail for This Example",
    "text": "Pre-validation in Detail for This Example\n\nDivide the cases up into \\(K = 13\\) equal-sized parts of 6 cases each.\nSet aside one of the parts. Using only the data from the other 12 parts:\n\nSelect the features having an absolute correlation of at least 0.3 with the class labels.\nForm a nearest centroid classification rule.\n\nUse the rule to predict the class labels for the 13th part.\nRepeat steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor \\(\\tilde{z}_i\\) for each of the 78 cases.\nFit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap versus Permutation Tests",
    "text": "The Bootstrap versus Permutation Tests\n\nBootstrap:\n\nSamples from the estimated population and uses the results to estimate standard errors and confidence intervals.\n\nPermutation Methods:\n\nSample from an estimated null distribution for the data.\nUsed to estimate p-values and False Discovery Rates for hypothesis tests.\n\nBootstrap for Null Hypothesis Testing:\n\nCan test a null hypothesis in simple situations.\nExample: If \\(\\theta = 0\\) is the null hypothesis, check whether the confidence interval for \\(\\theta\\) contains zero.\n\nAdapting Bootstrap for Null Distribution:\n\nCan adapt bootstrap to sample from a null distribution.\nSee Efron and Tibshirani, An Introduction to the Bootstrap (1993), Chapter 16.\nHowever, there is no real advantage over permutation tests."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#prediction",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#prediction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Prediction",
    "text": "Prediction\n\nGoal: Build predictors and classifiers to make accurate predictions from data.\nChallenge: How do we evaluate our predictions?\n\n\nIdeal Scenario: New Data\n\nThe best way to test predictions is to use new, independent data from the population.\nProblem: New data isn’t always available."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-not-use-training-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-not-use-training-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Not Use Training Data?",
    "text": "Why Not Use Training Data?\n\nUsing training data for evaluation is not reliable.\n\nModels tend to perform better on data they’ve already seen.\nThis leads to overly optimistic results."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#solution-resampling-methods",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#solution-resampling-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Solution: Resampling methods",
    "text": "Solution: Resampling methods\n\nCross-validation and the Bootstrap are two resampling methods.\nThese methods allows us to evaluate the performance of our predictors using the available data without relying on additional samples.\nThey refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nThe training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#precision-and-accuracy",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#precision-and-accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Precision and Accuracy",
    "text": "Precision and Accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision: Refers to the consistency or reliability of the model’s predictions.\nAccuracy: Refers to how close the model’s predictions are to the true values.\n\n\nIn the context of regression:\n\nHigh Precision, Low Accuracy: Predictions are consistent but biased.\nHigh Precision, High Accuracy: Predictions are both consistent and valid.\nLow Precision, Low Accuracy: Predictions are neither consistent nor valid.\nLow Precision, High Accuracy: Predictions are valid on average but have high variability."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-6",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error.\n\nThis is typically assessed using the Mean Squared Error (MSE) in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-5",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 3\\) here).\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation: in algebra",
    "text": "K-Fold Cross-Validation: in algebra\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute the cross-validations error rate:\n\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSpecial case: Setting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-auto-data-revisited",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Auto Data Revisited",
    "text": "Example: Auto Data Revisited\n\n\nLeft plot: Similar to the two halve validation;\nRight plot: Tenfold cross validation. With 10 different partitions of the data to train and test the model we see there is not much variability. The results are consistent, in contrast to the result when we divided into two parts."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#potential-issues-with-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#potential-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Potential Issues with Cross-Validation",
    "text": "Potential Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\n\nCompute the cross-validation misclassification error:\n\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?\n\nWe compute the standard errors assuming these were independent observations, but they are not strictly independent as they share some training samples. So there’s some correlation between them."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-3",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-setting",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-setting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Setting",
    "text": "The Setting\n\nHigh‐dimensional data: We have 50 samples (observations) but 5000 predictors (features). In many modern applications—such as genomics—it is typical to have many more predictors than observations.\nGoal: Two‐class classification\nFeature selection (Step 1): We first look at the correlation of each of the 5000 predictors with the class labels, and we pick the 100 “best” predictors—the ones that exhibit the largest correlation with the class labels.\nModel fitting (Step 2): Once those top 100 are chosen, we fit a classifier (e.g., logistic regression) using only those top 100 predictors.\nThe question is how to estimate the true test error of this two‐step procedure."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-tempting-but-wrong-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-tempting-but-wrong-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Tempting (but Wrong) Approach",
    "text": "The Tempting (but Wrong) Approach\nA common mistake is to ignore Step 1 when doing cross‐validation and to apply cross‐validation only to Step 2. That is, one might simply take the already‐selected 100 features and then do, say, 10‐fold cross‐validation on the logistic regression.\n\nWhy people do this: It seems natural to say, “Now that we have our 100 features, let’s cross‐validate the classifier we fit with these 100 features.”\nWhat goes wrong: By the time you pick those 100 “best” features, the data set has already “seen” all the labels in the process of ranking and filtering. This filtering step is actually part of training, because it used the outcome labels to choose features.\n\n\nSkipping Step 1 in the cross‐validation will invariably produce an overly optimistic (often wildly optimistic) estimate of test error."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-it-is-wrong-data-leakage",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-it-is-wrong-data-leakage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why It Is Wrong: Data Leakage",
    "text": "Why It Is Wrong: Data Leakage\n\nData leakage: The crucial point is that feature selection (filtering) depends on the relationship between each feature and the class labels. Hence, it is not “just a preprocessing step”—it is using the label information. Thus, Step 1 is part of the model‐building process.\nOverfitting by cherry‐picking: With thousands of predictors, even if none is truly predictive, by sheer chance some predictors will appear correlated with the class labels in the sample. Selecting only the strongest correlations can give the illusion that the model has learned meaningful structure, when in fact it is just capturing random noise.\nAn extreme illustration: If you simulate data where the class labels are purely random (true error = 50%), but you pick the top 100 out of 5000 or 5 million random features, then do cross‐validation only after you have chosen those top 100, you can easily see cross‐validation estimates near 0% error—clearly a false, biased result."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Correct (Right) Way to Apply Cross‐Validation",
    "text": "The Correct (Right) Way to Apply Cross‐Validation\n\nThe key principle is that any step that uses the outcome labels must occur inside the cross‐validation loop. Concretely:\n\nSplit the data into training/validation folds (e.g., 10‐fold CV).\nFor each fold:\n\nTreat that fold as a hold‐out set.\n\nOn the remaining training folds, perform the entire procedure:\n\nFeature selection (filtering to the top 100 based on correlation with the class labels in the training folds only).\n\nFit the classifier (e.g., logistic regression) to those top 100 features in those training folds.\n\n\nFinally, evaluate the trained model on the hold‐out fold—with only the 100 features selected from the training folds.\n\nRepeat for each fold, then average the error rates (or other metrics).\n\n\nBy doing this, each hold‐out fold is kept separate from both feature selection and model training. This ensures that Step 1 (feature selection) is “relearned” anew in each training subset, just as Step 2 (the classifier) is. As a result, the cross‐validation error you compute properly reflects how the entire procedure—from filtering out thousands of features down to fitting the logistic model—would perform on truly unseen data."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWrong: Select your 100 predictors once using all the data, then cross‐validate only the final classifier. This leads to overly optimistic, biased estimates of test error because it ignores that you used the labels in selecting those 100 predictors.\nRight: Wrap the entire two‐step process (selection and model fitting) inside the cross‐validation loop. Each fold’s feature‐selection step must be done without knowledge of the hold‐out fold’s labels.\n\n\nFollowing this correct approach is essential whenever one performs early filtering, variable selection, hyperparameter tuning, or any other step that uses the outcome labels. Such steps must be regarded as part of the training process and repeated inside each cross‐validation iteration."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-2",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nResampling Methods\n\nCross-validation and Bootstrap allow evaluation of model performance using existing data.\nThey provide estimates of:\n\nTest-set prediction error\nStandard deviation and bias of parameter estimates.\n\n\nTraining vs Test Error\n\nTraining error decreases with model complexity.\nTest error decreases, then increases due to bias-variance tradeoff:\n\nHigh Bias: Simple models underfit the data.\nHigh Variance: Complex models overfit the training data.\n\nOptimal complexity minimizes test error.\n\n\nValidation-Set Approach\n\nDivides data into training and validation sets.\nValidation error provides an estimate of test error but:\n\nCan vary based on data split.\nMay overestimate test error due to smaller training sets.\n\n\nCross-Validation\n\nK-Fold Cross-Validation:\n\nDivides data into \\(K\\) folds for iterative training and testing.\nBalances bias and variance (e.g., \\(K = 5\\) or \\(10\\)).\n\nLeave-One-Out Cross-Validation (LOOCV):\n\nUses one data point as validation in each iteration.\nLow bias but high variance.\n\n\nBootstrap\n\nEstimates variability and uncertainty of parameter estimates.\nGenerates multiple samples with replacement from the dataset.\nProvides approximate confidence intervals and standard errors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-results",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Results",
    "text": "Example Results\n\nLeft: A histogram of the estimates of \\(\\alpha\\) obtained by generating 1,000 simulated data sets from the true population.\nCenter: A histogram of the estimates of \\(\\alpha\\) obtained from 1,000 bootstrap samples from a single data set.\nRight: The estimates of \\(\\alpha\\) displayed in the left and center panels are shown as boxplots.\n\nIn each panel, the pink line indicates the true value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\n\nThere is a clear separation, no overlap, between the train and the test sets.\n\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample.\nThis will cause the bootstrap to seriously underestimate the true prediction error.\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example",
    "text": "Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha) Y).\\)\nOne can show that the value that minimizes the risk is given by:\n\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\]\nwhere \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Model Selection and Regularization\nSubset Selection\nStepwise Selection\nForward Stepwise Selection\nBackward Stepwise Selection\nChoosing the Optimal Model\nIndirect Approaches\nValidation and Cross-Validation\n\n\n\nShrinkage Methods\nRidge Regression\nThe Lasso\nSelecting the Tuning Parameter for Ridge Regression and Lasso\nDimension Reduction Methods\nPrincipal Components Regression\nPartial Least Squares (PLS)"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\n\nRecall the linear model\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. In the lectures covering Chapter 7 of the text, we generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In praise of linear models!",
    "text": "In praise of linear models!\n\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\nHence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why consider alternatives to least squares?",
    "text": "Why consider alternatives to least squares?\n\nPrediction Accuracy: especially when \\(p &gt; n\\), to control the variance.\nModel Interpretability: By removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing feature selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three classes of methods",
    "text": "Three classes of methods\n\nSubset Selection. We identify a subset of the \\(p\\) predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.Best Subset Selection, Foward Selection, and Backwards Selection are the main techniques here.\nShrinkage. We fit a model involving all \\(p\\) predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance and can also perform variable selection. Ridge Regression and Lasso are the main techniques here.\nDimension Reduction. We project the \\(p\\) predictors into a \\(M\\)-dimensional subspace, where \\(M &lt; p\\). This is achieved by computing \\(M\\) different linear combinations, or projections, of the variables. Then these \\(M\\) projections are used as predictors to fit a linear regression model by least squares. Principal Components Regression and Partial Least Squares are the main techniques here."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example - Credit data set",
    "text": "Example - Credit data set\n\n\n\n\n\n\n\n\n\n\n\nFor each possible model containing a subset of the ten predictors in the Credit data set, the Residual Sum of Squares (RSS) and \\(R^2\\) are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nThough the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables.\nThe reason that there’s a lot of dots in this picture is because there’s a lot of possible sub models given 10 total predictors. We have \\(2^{p} = 2^{10}\\approx 1,000\\) subsets.\nThe number \\(2^p\\) arises because each predictor (out of \\(p\\) predictors) can either be included or excluded from a subset model. This binary decision for each predictor gives \\(2\\) choices (include or exclude). When there are \\(p\\) predictors, the total number of possible subsets (or models) is calculated as \\(2^p\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions to other models",
    "text": "Extensions to other models\n\n\n\nThe same ideas apply to other types of models, such as logistic regression.\nWhen dealing with other type of models, instead of the RSS, we look into the deviance (D), which is commonly used in generalized linear models. The deviance is calculated as:\n\n\\[\nD = -2 \\cdot \\log L_{\\text{max}}\n\\]\nwhere:\n\n\\(D\\) is the deviance,\n\\(\\log L_{\\text{max}}\\) is the maximized log-likelihood of the model.\n\nThis formula allows the deviance to serve as a measure of goodness of fit, analogous to the residual sum of squares (RSS) in linear regression, but applicable to a broader class of models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\). Why not?\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nIn particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In Detail",
    "text": "In Detail\nForward Stepwise Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Forward Stepwise Selection",
    "text": "More on Forward Stepwise Selection\n\nComputational advantage over best subset selection is clear.\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors. Why not? Give an example."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set.\n\n\n\n\n\n\n\n\n# Variables\nBest subset\nForward stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\nThe first three models are identical but the fourth models differ.\nThis discrepancy happens because there is correlation between features."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: details",
    "text": "Backward Stepwise Selection: details\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all \\(p\\) predictors.\nFor \\(k = p, p - 1, \\ldots, 1\\):\n\n2.1 Consider all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k - 1\\) predictors.\n2.2 Choose the best among these \\(k\\) models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Backward Stepwise Selection",
    "text": "More on Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating test error: two approaches",
    "text": "Estimating test error: two approaches\n\nIndirect: We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nDirect: We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)\nThese techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.\n\nThe figure displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set.\n\nIt suggests that we must choose a model with 4 to 6 predictors.\nThe main recommmendation is to keep the model as simple as possible. By identifying that the values do not change too much as we increase the number of predictors, a model with 4 predictors will be recommended."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10\\) folds. In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule.\n\nEstimate Test Error: We compute the test error (e.g., MSE) for each model size.\nCalculate Standard Error: Compute the standard error (SE) of the test error for each model size to account for variability.\nSelect the Model: Identify the model with the lowest test error (the “best” model). Choose the simplest model whose test error is within one SE of the lowest test error."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now for some details",
    "text": "Now for some details\n\nMallow’s \\(C_p\\): \\[\nC_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\n\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement.\n\nThe AIC criterion is defined for a large class of models fit by maximum likelihood:\n\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent. Prove this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details on BIC",
    "text": "Details on BIC\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\nFor a least squares model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as\n\\[\n    \\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n - d - 1)}{\\text{TSS}/(n - 1)}.\n\\]\nwhere TSS is the total sum of squares, \\(TSS = \\Sigma_i^n(y_i - \\bar{y})^2\\).\n\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{\\text{RSS}}{n - d - 1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{\\text{RSS}}{n - d - 1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\). As \\(\\lambda\\) increases, it pushes the coefficients towards zero.\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\).\nIn the right-hand panel, when \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2 = 1\\), \\(\\lambda = 0\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize \\[\n\\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression: continued",
    "text": "Ridge regression: continued\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\). \\(\\lambda = 0.05\\) minimizes the cross-validation error.\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge Regression: Scaling of Predictors",
    "text": "Ridge Regression: Scaling of Predictors\n\nThe standard least squares coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}}\n\\]\n\n\nAfter standardizing the predictors, their standard deviations will be 1. That make the features comparable and make the coefficients comparable."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\nThe Bias-Variance Tradeoff\n\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nSquared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\).\nThe horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses an \\(\\ell_1\\) (pronounced “ell 1”) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso: Continued",
    "text": "The Lasso: Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Dataset",
    "text": "Example: Credit Dataset"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?\nOne can show that the lasso and ridge regression coefficient estimates solve the problems (equivalent to Lagrange formulations):\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} |\\beta_j| \\leq s\n\\]\nand\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s,\n\\]\nrespectively."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso Picture",
    "text": "The Lasso Picture"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nLeft: Lasso: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression: continued",
    "text": "Comparing the Lasso and Ridge Regression: continued\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data equals to the one used before, except that now only two predictors are related to the response.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\nCross-validation errors for ridge regression\n\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\).\n\n\n\n\nCoefficient estimates as function of lambda\n\n\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Data Example",
    "text": "Simulated Data Example\n\nLeft: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set.\nRight: The corresponding lasso coeﬃcient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Details",
    "text": "Dimension Reduction Methods: Details\n\n\n\nLet \\(Z_1, Z_2, \\ldots, Z_M\\) represent \\(M &lt; p\\) linear combinations of our original \\(p\\) predictors. That is, \\[\nZ_m = \\sum_{j=1}^p \\phi_{mj} X_j \\quad \\text{(1)}\n\\]\n\nfor some constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\).\n\nWe can then fit the linear regression model,\n\n\\[\n    y_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\quad \\text{(2)}\n\\]\nusing ordinary least squares.\n\nNote that in model (2), the regression coefficients are given by \\(\\theta_0, \\theta_1, \\ldots, \\theta_M\\). If the constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\) are chosen wisely, then such dimension reduction approaches can often outperform OLS regression."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Continued",
    "text": "Dimension Reduction Methods: Continued\n\nNotice that from definition (1),\n\n\\[\n    \\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where\n\\[\n    \\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\n\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component.\nNote that these two principal componenets are uncorrelated!"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nA subset of the advertising data. Left: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. Right: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application to Principal Components Regression",
    "text": "Application to Principal Components Regression\n\nPCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.\n\nLeft: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors. The plot shows that a model with \\(\\approx 18\\) principal components can provide a good result.\nRight: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors, except that now only two predictors are related to the response. The plot shows that a model with \\(\\approx 25\\) principal components can provide a good result."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Directions \\(M\\)",
    "text": "Choosing the Number of Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares: Continued",
    "text": "Partial Least Squares: Continued\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Partial Least Squares",
    "text": "Details of Partial Least Squares\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nEstimating test error for models involves adjusting training error or using direct methods like cross-validation. Tools like CP, AIC, BIC, and adjusted R-squared help select optimal models.\nHighlights\n\nEstimating test error is crucial for model selection.\nTwo approaches: indirect adjustment of training error and direct estimation methods.\nCP, AIC, BIC, and adjusted R-squared help model comparison.\nA model should minimize CP and BIC while maximizing adjusted R-squared.\nAdjusted R-squared allows meaningful comparisons across different models.\nCross-validation is versatile and can be applied to various models.\nSimplicity is favored; fewer predictors often yield better results.\n\nKey Insights\n\nTest Error Estimation: Accurate test error estimation is vital for model evaluation. It helps choose the best model among multiple options, ensuring reliability in predictions.\nIndirect vs. Direct Methods: Understanding both indirect (adjusting training error) and direct (cross-validation) methods provides flexibility in model evaluation, catering to different scenarios in data analysis.\nModel Selection Criteria: CP, AIC, BIC, and adjusted R-squared serve as essential criteria for model selection. They help quantify model performance and complexity, aiding in decision-making.\nMinimizing CP and BIC: Aiming for lower CP and BIC values suggests a more parsimonious model, which is often preferred for its simplicity and interpretability while still capturing the necessary relationships.\nCross-Validation Versatility: Cross-validation is a powerful tool applicable to a wide range of models, including non-linear ones, making it a preferred method for estimating test error in various contexts.\nAdjusted R-squared Utility: Unlike traditional R-squared, adjusted R-squared provides a way to compare models with differing numbers of predictors, addressing the limitations of model evaluation in regression analysis.\nSimplicity Preference: Favoring simpler models with fewer predictors can lead to better generalization and reduced risk of overfitting, aligning with the principle of Occam’s Razor in statistical modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nShrinkage methods like Ridge regression and Lasso use penalties to shrink coefficients towards zero, improving model performance, especially with large datasets.\nHighlights\n\nRidge regression uses a penalty to shrink coefficients towards zero.\nLasso also shrinks coefficients but can set some to exactly zero.\nThese methods are effective for large datasets with many variables.\nFast computation has revived interest in these techniques recently.\nCross-validation is crucial for selecting the optimal tuning parameter, Lambda.\nScaling of variables is important when applying Ridge regression.\nRidge regression reduces variance while maintaining bias, leading to better mean squared error.\n\nKey Insights\n\nShrinkage Techniques: Ridge regression and Lasso are modern approaches to regularization, balancing model fit and complexity. Shrinking coefficients helps mitigate overfitting, particularly in high-dimensional data.\nTuning Parameter Lambda: The choice of Lambda is critical; it determines the strength of the penalty. Using cross-validation to optimize this parameter is essential for achieving the best model performance.\nBias-Variance Tradeoff: Ridge regression effectively controls variance without significantly increasing bias, thereby minimizing mean squared error. This tradeoff is vital for model accuracy.\nLarge Datasets: As datasets grow in size and complexity, shrinkage methods become increasingly relevant. They are designed to handle situations where the number of predictors can exceed the number of observations.\nImportance of Scaling: Unlike least squares, the performance of Ridge regression is sensitive to the scale of the predictors. Standardizing variables ensures comparability and effectiveness of the shrinkage.\nContinuous Shrinkage: Ridge regression produces coefficients that are close to zero but rarely exactly zero, which differs from Lasso. This characteristic can be advantageous for retaining all predictors in the model.\nCurrent Research Trends: Shrinkage methods are a hot topic in statistical research, with ongoing developments aimed at enhancing their effectiveness and applicability across various fields."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\nRecall the linear model\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. We will generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#best-subset-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#best-subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nThe core idea is to identify a simpler model that includes only a subset of the \\(P\\) available predictors, thereby improving interpretability and potentially enhancing predictive performance.\nTo implement best subset selection systematically, we consider every possible combination of predictors and evaluate each resulting model. The process begins with the null model (\\(M_0\\)), which includes no predictors and only an intercept, meaning it predicts the sample mean for all observations. From there, models are incrementally built by incorporating different subsets of predictors, ultimately selecting the model that optimally balances predictive accuracy and complexity. Here are the steps:\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\nFit all \\(\\binom{p}{k}\\) models, “\\(p\\) choose \\(k\\) models”, that contain exactly \\(k\\) predictors. \\(\\binom{p}{k} = \\frac{p!}{k!(p-k)!}\\)\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest Residual Sum of Squares (RSS), or equivalently the largest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\). The goal is to choose the model with the smallest test error, not the smallest training error."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\).\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For the authors of the book, it is not recommended to use the best subset approach if you have more than 20 predictors.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models (\\(p^2\\)), are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-in-detail",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection: In Detail",
    "text": "Forward Stepwise Selection: In Detail\n\n\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor. This is different from what we were doing in in the best subset selection case. Here do not look at every possible model containing \\(p\\) predictors. Instead, we are just looking at every possible model that contains one more predictor than \\(M_{k-1}\\).\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\).\n\n\nForward Stepwise Selection presents computational advantage over best subset selection. The total number of models evaluated is the sum of the sequence:\n\n\\[\np + (p - 1) + (p - 2) + \\dots + 1 = \\frac{p(p + 1)}{2}\n\\]\n\nFor large \\(p\\), the term \\(\\frac{p(p + 1)}{2}\\) is dominated by \\(\\frac{p^2}{2}\\). Thus, the computational cost is approximately proportional to \\(p^2\\).\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection: Summary",
    "text": "Forward Stepwise Selection: Summary\n\n\nStepwise selection is a computationally efficient alternative to best subset selection in model building, especially with large predictor sets.\nHighlights\n\nStepwise selection offers a practical approach for model selection.\nBest subset selection can lead to overfitting, especially with many predictors.\nForward stepwise selection considers fewer models than best subset, making it computationally efficient.\nDeviance generalizes residual sum of squares across various models.\nBest subset selection becomes impractical beyond 30-40 predictors due to computational limits.\nForward stepwise may not always find the optimal model compared to best subset.\nCorrelation between features impacts model selection outcomes between methods.\n\nKey Insights\n\nComputational Efficiency: Stepwise selection significantly reduces the number of models evaluated, making it feasible for larger datasets. This is essential in modern data analysis, where predictors can number in the thousands.\nOverfitting Risks: With best subset selection, the risk of overfitting increases as the number of predictors grows, which can lead to poor performance on unseen data. This highlights the importance of model validation techniques.\nModel Nesting: Forward stepwise selection builds models incrementally, ensuring that each new model is a superset of the previous one, which helps maintain a streamlined search process for the best predictors.\nDeviance vs. RSS: Understanding the difference in metrics like deviance and residual sum of squares is crucial for accurately assessing model fit across various types of regression analyses.\nPractical Limits: Most statistical packages struggle with subset selection beyond 30-40 predictors, indicating the need for streamlined methods like stepwise selection in high-dimensional contexts.\nModel Comparison: Forward stepwise selection may yield different models than best subset selection, emphasizing the need for careful evaluation of model performance on independent datasets.\nCorrelation Effects: The discrepancies between the two methods arise from correlations among predictors, showcasing the intricate dynamics of variable selection in regression modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: Summary",
    "text": "Backward Stepwise Selection: Summary\n\n\nBackward stepwise selection removes predictors from a full model to improve efficiency in model selection, contrasting with forward stepwise selection.\nHighlights\n\nBackward stepwise starts with a full model (mp) and removes predictors one at a time.\nIt evaluates the least useful predictor to minimize impact on model fit.\nThis method is computationally efficient, considering around p²/2 models.\nOnly applicable when the number of observations (n) is greater than the number of predictors.\nR-squared and RSS might mislead model selection, focusing on training error rather than test error.\nCross-validation, AIC, BIC, or adjusted R-squared should guide the final model choice.\nBackward and forward selections are not guaranteed to find the best model but can yield good test set results.\n\nKey Insights\n\nMethodology Contrast: Backward stepwise selection is an efficient alternative to forward selection, emphasizing the removal of predictors rather than their addition. This reversal highlights different strategies in model optimization.\nModel Evaluation: The approach assesses the least impactful predictors, ensuring that model performance remains stable as predictors are eliminated, which is crucial for maintaining predictive accuracy.\nComputational Efficiency: Backward stepwise selection dramatically reduces computational load compared to best subset selection, making it a suitable option for larger datasets.\nObservational Requirement: This method necessitates that the number of observations is greater than the number of predictors, ensuring that a least squares model can be appropriately fitted, which is a critical consideration in practical applications.\nTraining vs. Test Error: Relying solely on training error metrics like RSS and R-squared can lead to overfitting, indicating the need for broader evaluation methods to predict future performance.\nModel Selection Techniques: Utilizing techniques like cross-validation, AIC, or BIC for model selection can help mitigate the risks associated with simply opting for models with the best training error.\nOutcome Consistency: While backward stepwise may not find the absolute best model, it can produce models that perform well on unseen data, demonstrating its practical utility in predictive modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\n\nMallows’ \\(C_p\\) balances model fit and model complexity:\n\\[\nC_p = \\frac{1}{n} \\left( \\text{RSS} + 2d\\hat{\\sigma}^2 \\right)\n\\]\nwhere:\n\n\\(d\\): Total number of parameters used in the model (including the intercept).\n\n\\(\\hat{\\sigma}^2\\): Estimate of the error variance \\(\\epsilon\\), associated with each response measurement.\n\n\\(\\text{RSS}\\): Residual Sum of Squares, measuring the error between observed and predicted values.\n\n\\(n\\): Number of observations in the dataset.\n\nExplanation\n\nThe \\(\\text{RSS}\\) measures the fit.\nThe penalty term \\(2d\\hat{\\sigma}^2\\) discourages overfitting.\n\nDecision: The lowest, the better!"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#aic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#aic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AIC",
    "text": "AIC\nThe Akaike Information Criteria (AIC) criterion is defined for a large class of models fit by maximum likelihood:\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model, \\(-2 \\log L = \\frac{RSS}{\\sigma^2}\\)\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent.\nAIC and Mallow’s \\(C_p\\) are proportional to each other.\nAIC is a good approach for non-linear models, e.g. logistic regression."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#bic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BIC",
    "text": "BIC\nThis is the Bayesian Information Criterion (BIC):\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\) or AIC."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mallow’s \\(C_p\\):",
    "text": "Mallow’s \\(C_p\\):\n\\[\n    C_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts\n\nThree classes of methods:\n\nSubset Selection: Focuses on identifying subsets of predictors for simpler models.\nShrinkage Methods: Regularizes coefficients to reduce variance and improve model performance (e.g., Ridge, Lasso).\nDimension Reduction: Reduces the number of predictors using linear combinations (e.g., PCA, PLS).\n\nModel Selection Criteria:\n\nUse metrics like $C_p $, AIC, BIC, and Adjusted $R^2 $to balance model fit and complexity.\nCross-validation is essential for estimating test error and selecting tuning parameters.\n\nBias-Variance Tradeoff:\n\nShrinkage methods improve prediction accuracy by reducing variance while maintaining bias.\n\n\n\nPractical Insights\n\nBest Subset Selection:\n\nComputationally intensive (\\(2^p\\) models).\nNot recommended for \\(p &gt; 20\\).\n\nStepwise Selection:\n\nMore efficient (\\(p^2\\) models).\nForward and backward methods balance performance and computation.\n\nRidge vs. Lasso:\n\nRidge shrinks coefficients but includes all predictors.\nLasso performs variable selection by setting coefficients to zero.\n\nPrincipal Components Regression (PCR):\n\nReduces dimensionality by finding uncorrelated components.\nWorks well when high variance directions correlate with the response.\n\nPartial Least Squares (PLS):\n\nSupervised alternative to PCR, incorporating response variable information."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize\n\\[\n    \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\n\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize\n\n\\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n    = \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\]\nwhere \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately.\n\nShrinkage penalty: \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\) penalizes coefficients that get too large. The model will pay a price according to the number of non-zero coefficients."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-4",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nSelecting the tuning parameter (lambda) for ridge regression and lasso is crucial, as it significantly influences model performance. Cross-validation is an effective method for this selection.\nHighlights\n\nLambda is crucial: Affects the solution from full least squares to zero coefficients. ⚖️ Regularization importance: Zero lambda means no regularization; high lambda leads to zero solutions.\nCross-validation advantage: Ideal for tuning parameters as it doesn’t require the unknown number of parameters (d).\nRidge example: With lambda of 100, all variables appear included, but coefficients are shrunk.\nCross-validation curves: Show how errors change with varying lambda values for both ridge and lasso.\nLasso effectiveness: Properly identifies non-zero coefficients while setting others to zero.\nSimulation success: In a simulated scenario, the model accurately identifies the correct number of non-zero coefficients.\n\nKey Insights\n\nImportance of Lambda: The tuning parameter lambda significantly influences the model’s complexity and overall performance. Choosing lambda wisely is essential for achieving the desired balance between bias and variance.\nCross-validation as a solution: Cross-validation provides a robust framework for assessing model performance across different lambda values without needing to know the exact number of parameters, making it a practical choice for tuning.\nDegree of freedom confusion: In ridge regression, even when coefficients are shrunk, counting parameters can be misleading, as all variables remain included in the model. ⚖️ Regularization trade-offs: The process of regularization through ridge and lasso not only simplifies models but also introduces nuanced definitions of model complexity, changing our understanding of ‘degrees of freedom.’\nError analysis via curves: Cross-validation curves reveal how model errors fluctuate with lambda, helping visualize optimal tuning points.\nLasso’s precision: Lasso regression demonstrates its strength in feature selection, effectively pinpointing relevant variables while ignoring the irrelevant ones, enhancing interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\n\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso, first published in 1996 by Rob Tibshirani, one of the authors of the book, is an alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses the sum of absolute values, an \\(\\ell_1\\) (pronounced “ell 1”) penalty, instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection. It is a combination of shirinkage and selection of variables.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-and-ridge-picture",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-and-ridge-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso and Ridge Picture",
    "text": "The Lasso and Ridge Picture\nThis picture helps to explain why the lasso gives sparsity:\n\n\nOn the right we have the ridge regression and on the left is the lasso regression.\nIt is possible to see where the where the red boundary touch the blue constraing.\nIn the case of the ridge regression (right plot), we see that the solution does not create a zero.\nIn the case of the lasso regression (left plot), we see that the solution does create a zero for one of the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nThe Lasso regression technique improves upon ridge regression by both shrinking coefficients and performing variable selection, setting some coefficients to zero.\nHighlights\n\nLasso regression shrinks coefficients while allowing for variable selection.\nIt uses an L1 penalty, contrasting with ridge’s L2 penalty.\nThe concept of sparsity is central to Lasso’s effectiveness.\nIncreased computational efficiency has popularized Lasso in recent years.\nLasso is particularly useful in high-dimensional datasets with many features.\nCross-validation is essential for selecting the optimal lambda value.\nPerformance varies: Lasso excels in sparse models, while ridge may perform better in dense ones.\n\nKey Insights\n\nLasso vs. Ridge: Lasso regression not only shrinks coefficients but also sets some to zero, enabling simpler models through variable selection. This property makes it particularly valuable in high-dimensional settings where many variables may be irrelevant.\nL1 vs. L2 Penalty: The L1 penalty used in Lasso creates a constraint that promotes sparsity, while the L2 penalty in ridge regression tends to retain all variables with smaller coefficients. This difference is crucial for effective model building.\nSparsity: The concept of sparsity refers to models that only include a small subset of variables. Sparse models are easier to interpret and can enhance predictive performance when only a few predictors are relevant.\nComputational Advances: Recent improvements in computational power and techniques in convex optimization have made applying Lasso feasible even on large datasets, broadening its applicability across various fields.\nReal-World Applications: In situations like medical diagnostics, where finding a minimal number of significant predictors is vital, Lasso provides a practical solution by efficiently identifying key variables among thousands of measurements.\nChoosing Lambda: The tuning parameter lambda is critical; cross-validation is typically used to determine its optimal value, balancing model complexity and predictive accuracy.\nModel Performance: The effectiveness of Lasso and ridge regression varies based on the underlying data structure. Lasso performs better with sparse true models, while ridge regression may be more effective when many predictors are significant."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-5",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nDimension reduction transforms original predictors into fewer linear combinations, improving model fitting while maintaining low bias and variance.\nHighlights\n\nDimension Reduction simplifies models by using fewer predictors.\nNew predictors are linear combinations of original ones.\nFitting uses least squares on transformed predictors.\nAim: Reduce dimensions from P predictors to M (M &lt; P).\nBalances bias and variance effectively.\nSimilar to Ridge and Lasso, but with different coefficient constraints.\nWorks best when M &lt; P; otherwise, it results in standard least squares.\n\nKey Insights\n\nEfficiency in Modeling: Dimension reduction allows for a simpler model with fewer predictors, leading to potentially better performance without losing significant information. This method is advantageous in high-dimensional datasets.\nConstruction of New Predictors: By creating new predictors through linear combinations, we can capture essential relationships in the data while reducing complexity, which may help in enhancing interpretability.\nBias-Variance Trade-off: This approach effectively manages the bias-variance trade-off, leading to models with lower bias and variance compared to using all original features, which is crucial for better generalization to unseen data.\nUse of Least Squares: While retaining the least squares fitting method, this approach modifies the predictor space, allowing for a fresh perspective on regression problems and leading to potentially improved outcomes.\nRelation to Ridge and Lasso: Although dimension reduction shares similarities with Ridge and Lasso in terms of model fitting, it introduces unique constraints on coefficients, which can lead to different insights about the data.\nImportance of Dimensions: The effectiveness of dimension reduction hinges on the condition that M (new predictors) is less than P (original predictors). If M equals P, the method reduces to standard least squares, negating its advantages.\nInnovation in Coefficient Form: The requirement for coefficients to adopt a specific structure in dimension reduction can provide insights into the relationships among predictors, enhancing model interpretability and utility."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#conclusions-about-ridge-and-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#conclusions-about-ridge-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions about Ridge and Lasso",
    "text": "Conclusions about Ridge and Lasso\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-6",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nPrincipal Components Regression (PCR) reduces dimensionality by finding principal components and using them in least squares regression for efficient modeling.\nHighlights\n\nPrincipal Components Regression (PCR) uses a two-step procedure to reduce dimensionality.\nFirst, principal components with the highest variance are identified from the data.\nThe first principal component is aligned with the direction of maximum variance.\nThe second principal component is uncorrelated with the first and captures additional variance.\nUsing few principal components can effectively summarize complex datasets.\nChoosing the optimal number of components is crucial for minimizing mean squared error.\nPartial Least Squares (PLS) improves upon PCR by considering response variables in component selection.\n\nKey Insights\n\nDimensionality Reduction: PCR simplifies models by reducing the number of predictors while retaining essential information, aiding in interpretation and computation. This is particularly useful with datasets containing many variables relative to observations.\nUncorrelated Components: The process ensures that the principal components are uncorrelated, which helps in creating more robust models by minimizing multicollinearity issues common in regression analysis.\nModel Selection: The selection of the number of components directly impacts model performance. Cross-validation is recommended to find the optimal number of components for the best predictive accuracy.\nEfficiency in Prediction: PCR can significantly enhance prediction accuracy when dealing with high-dimensional data by focusing on variance rather than individual variable contributions.\nAssumption of Variance-Response Relationship: The effectiveness of PCR hinges on the assumption that high variance directions in predictors correlate with the response, which may not always hold true.\nPartial Least Squares: PLS offers a supervised alternative to PCR by incorporating response variable information, potentially leading to better predictive models, although it may not always outperform PCR.\nModern Applications: Techniques like PCR and PLS are increasingly relevant in fields with large datasets, where simpler models are needed to prevent overfitting and enhance interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-8",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-8",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts\n\nThree classes of methods:\n\nSubset Selection: Focuses on identifying subsets of predictors for simpler models.\nShrinkage Methods: Regularizes coefficients to reduce variance and improve model performance (e.g., Ridge, Lasso).\nDimension Reduction: Reduces the number of predictors using linear combinations (e.g., PCA, PLS).\n\nModel Selection Criteria:\n\nUse metrics like $C_p $, AIC, BIC, and Adjusted $R^2 $to balance model fit and complexity.\nCross-validation is essential for estimating test error and selecting tuning parameters.\n\nBias-Variance Tradeoff:\n\nShrinkage methods improve prediction accuracy by reducing variance while maintaining bias.\n\n\n\n\n\nPractical Insights\n\nBest Subset Selection:\n\nComputationally intensive ($2^p $models).\nNot recommended for $p &gt; 20 $.\n\nStepwise Selection:\n\nMore efficient ($p^2 $models).\nForward and backward methods balance performance and computation.\n\nRidge vs. Lasso:\n\nRidge shrinks coefficients but includes all predictors.\nLasso performs variable selection by setting coefficients to zero.\n\nPrincipal Components Regression (PCR):\n\nReduces dimensionality by finding uncorrelated components.\nWorks well when high variance directions correlate with the response.\n\nPartial Least Squares (PLS):\n\nSupervised alternative to PCR, incorporating response variable information."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nBy far the most famous dimension reduction approach. It involves a two-step procedure:\n\nStep 1: we find what are called principal components of the data matrix (linear combinations of the predictors)\nStep 2: we perform least squares regression using those principal components as predictors\n\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.\nThe intuition is that if you have a data set with 45 variables and compute a few principal components, those might capture most of the variation in the data."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\n\n\n\n\n\n\n\n\n\nA subset of the advertising data.\n\nLeft: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments.\nRight: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong.\n\nWe can visualize each principal component by plotting it against the original variables, such as population and ad spending.\nWe observe that the first principal component is highly correlated with both population and ad spending. This indicates that the first principal component effectively captures the variability in these two variables, summarizing the data in a meaningful way.\nThis suggests a valuable insight: instead of using the original variables (population and ad spending) directly, we can use the first principal component as a single, simplified predictor. We have the assumption that a linear combination of the predictors that has high variance is probably going to be associated with the response.\n\nFor example, if my goal is to predict a response variable like sales, we can incorporate the first principal component as a predictor in the model. This approach reduces the dimensionality of the data while retaining much of the information, potentially improving model interpretability and efficiency."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Principal Component Directions \\(M\\)",
    "text": "Choosing the Number of Principal Component Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\). For each of the models we can see the cross-validated mean squared error. Here we have disappointing result. If we pick a model for which the mean squared error is as small as possible, here the mean squared error is really as small as possible when we have a model with 10 or 11 components. However, in our dataset \\(M = 11\\) is going to be the regular least squares on the original data using all variables. Basically, principal components regression does not provide any gains in this case."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\nA potential solution is to use Partial Least Squares (PLS)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nPLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS): Details",
    "text": "Partial Least Squares (PLS): Details\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription.\nThe authors of the book highlight that PLS does not bring to much gain when compared to Ridge regression approach, for example."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Forward Stepwise Selection",
    "text": "Summary: Forward Stepwise Selection\n\n\nStepwise selection is a computationally efficient alternative to best subset selection in model building, especially with large predictor sets.\n\n\nHighlights\n\nStepwise selection offers a practical approach for model selection.\nBest subset selection can lead to overfitting, especially with many predictors.\nForward stepwise selection considers fewer models than best subset, making it computationally efficient.\nDeviance generalizes residual sum of squares across various models.\nBest subset selection becomes impractical beyond 30-40 predictors due to computational limits.\nForward stepwise may not always find the optimal model compared to best subset.\nCorrelation between features impacts model selection outcomes between methods.\n\n\nKey Insights\n\nComputational Efficiency: Stepwise selection significantly reduces the number of models evaluated, making it feasible for larger datasets. This is essential in modern data analysis, where predictors can number in the thousands.\nOverfitting Risks: With best subset selection, the risk of overfitting increases as the number of predictors grows, which can lead to poor performance on unseen data. This highlights the importance of model validation techniques.\nModel Nesting: Forward stepwise selection builds models incrementally, ensuring that each new model is a superset of the previous one, which helps maintain a streamlined search process for the best predictors.\nDeviance vs. RSS: Understanding the difference in metrics like deviance and residual sum of squares is crucial for accurately assessing model fit across various types of regression analyses.\nPractical Limits: Most statistical packages struggle with subset selection beyond 30-40 predictors, indicating the need for streamlined methods like stepwise selection in high-dimensional contexts.\nModel Comparison: Forward stepwise selection may yield different models than best subset selection, emphasizing the need for careful evaluation of model performance on independent datasets.\nCorrelation Effects: The discrepancies between the two methods arise from correlations among predictors, showcasing the intricate dynamics of variable selection in regression modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Backward Stepwise Selection",
    "text": "Summary: Backward Stepwise Selection\n\n\nBackward stepwise selection removes predictors from a full model to improve efficiency in model selection, contrasting with forward stepwise selection.\n\n\nHighlights\n\nBackward stepwise starts with a full model (\\(M_p\\)) and removes predictors one at a time.\nIt evaluates the least useful predictor to minimize impact on model fit.\nThis method is computationally efficient, considering around \\(p^2/2\\)models.\nOnly applicable when the number of observations (\\(n\\)) is greater than the number of predictors.\n\\(R^2\\)and RSS might mislead model selection, focusing on training error rather than test error.\nCross-validation, AIC, BIC, or adjusted \\(R^2\\)should guide the final model choice.\nBackward and forward selections are not guaranteed to find the best model but can yield good test set results.\n\n\nKey Insights\n\nMethodology Contrast: Backward stepwise selection is an efficient alternative to forward selection, emphasizing the removal of predictors rather than their addition. This reversal highlights different strategies in model optimization.\nModel Evaluation: The approach assesses the least impactful predictors, ensuring that model performance remains stable as predictors are eliminated, which is crucial for maintaining predictive accuracy.\nComputational Efficiency: Backward stepwise selection dramatically reduces computational load compared to best subset selection, making it a suitable option for larger datasets.\nObservational Requirement: This method necessitates that the number of observations is greater than the number of predictors, ensuring that a least squares model can be appropriately fitted, which is a critical consideration in practical applications.\nTraining vs. Test Error: Relying solely on training error metrics like RSS and \\(R^2\\)can lead to overfitting, indicating the need for broader evaluation methods to predict future performance.\nModel Selection Techniques: Utilizing techniques like cross-validation, AIC, or BIC for model selection can help mitigate the risks associated with simply opting for models with the best training error.\nOutcome Consistency: While backward stepwise may not find the absolute best model, it can produce models that perform well on unseen data, demonstrating its practical utility in predictive modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared",
    "text": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared\n\n\nEstimating test error for models involves adjusting training error or using direct methods like cross-validation. Tools like CP, AIC, BIC, and adjusted R-squared help select optimal models.\n\n\nHighlights\n\nEstimating test error is crucial for model selection.\nTwo approaches: indirect adjustment of training error and direct estimation methods.\n\\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) help model comparison.\nA model should minimize \\(C_p\\) and BIC while maximizing adjusted \\(R^2\\).\nAdjusted \\(R^2\\) allows meaningful comparisons across different models.\nCross-validation is versatile and can be applied to various models.\nSimplicity is favored; fewer predictors often yield better results.\n\n\nKey Insights\n\nTest Error Estimation: Accurate test error estimation is vital for model evaluation. It helps choose the best model among multiple options, ensuring reliability in predictions.\nIndirect vs. Direct Methods: Understanding both indirect (adjusting training error) and direct (cross-validation) methods provides flexibility in model evaluation, catering to different scenarios in data analysis.\nModel Selection Criteria: \\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) serve as essential criteria for model selection. They help quantify model performance and complexity, aiding in decision-making.\nMinimizing \\(C_p\\) and BIC: Aiming for lower \\(C_p\\) and BIC values suggests a more parsimonious model, which is often preferred for its simplicity and interpretability while still capturing the necessary relationships.\nCross-Validation Versatility: Cross-validation is a powerful tool applicable to a wide range of models, including non-linear ones, making it a preferred method for estimating test error in various contexts.\nAdjusted \\(R^2\\) Utility: Unlike traditional \\(R^2\\), adjusted \\(R^2\\) provides a way to compare models with differing numbers of predictors, addressing the limitations of model evaluation in regression analysis.\nSimplicity Preference: Favoring simpler models with fewer predictors can lead to better generalization and reduced risk of overfitting, aligning with the principle of Occam’s Razor in statistical modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-validation-and-cross-validation",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Validation and Cross-validation",
    "text": "Summary: Validation and Cross-validation\n\n\nValidation and cross-validation help select the best model size by estimating prediction error without needing sigma squared or the number of parameters.\n\n\nHighlights\n\nValidation splits data into training and validation sets for error estimation.\nCross-validation trains on multiple parts of data to improve error estimates.\nChoosing the optimal model size minimizes validation error effectively.\nAvoiding sigma squared estimation is crucial in high-dimensional data scenarios.\nThe one standard error rule favors simpler models that perform similarly to the best.\nBIC tends to prefer smaller models compared to AIC in error estimation.\nNew data challenges continuously evolve statistical methods and research.\n\n\nKey Insights\n\nModel Selection: Validation and cross-validation provide direct methods for estimating prediction error, making them essential for model selection. This ensures the chosen model performs well on unseen data.\nError Estimation: By dividing data into training and validation sets, we can effectively estimate how well a model will generalize, leading to more robust predictions in practice.\nAvoiding Estimation Challenges: In high-dimensional settings, traditional methods for estimating sigma squared and the number of parameters (\\(d\\)) can be unreliable. Cross-validation mitigates these concerns, simplifying the model selection process.\nSimplicity Preference: The one standard error rule encourages selecting simpler models that perform nearly as well as the best, enhancing interpretability and reducing overfitting.\nIterative Evaluation: Cross-validation’s iterative nature allows for more reliable error estimates by using multiple data partitions, thus improving the stability of model evaluations.\nBIC vs. AIC: BIC’s stronger penalty for model complexity often results in smaller models compared to AIC, which can lead to different model selection outcomes.\nEvolving Challenges: The increasing complexity of data in fields like high-dimensional statistics presents ongoing challenges, propelling research and innovation in statistical methodologies."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-shrinkage-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Shrinkage methods",
    "text": "Summary: Shrinkage methods\n\n\nShrinkage methods like Ridge regression and Lasso use penalties to shrink coefficients towards zero, improving model performance, especially with large datasets.\n\n\nHighlights\n\nRidge regression uses a penalty to shrink coefficients towards zero.\nLasso also shrinks coefficients but can set some to exactly zero.\nThese methods are effective for large datasets with many variables.\nFast computation has revived interest in these techniques recently.\nCross-validation is crucial for selecting the optimal tuning parameter, Lambda.\nScaling of variables is important when applying Ridge regression.\nRidge regression reduces variance while maintaining bias, leading to better mean squared error.\n\n\nKey Insights\n\nShrinkage Techniques: Ridge regression and Lasso are modern approaches to regularization, balancing model fit and complexity. Shrinking coefficients helps mitigate overfitting, particularly in high-dimensional data.\nTuning Parameter Lambda: The choice of Lambda is critical; it determines the strength of the penalty. Using cross-validation to optimize this parameter is essential for achieving the best model performance.\nBias-Variance Tradeoff: Ridge regression effectively controls variance without significantly increasing bias, thereby minimizing mean squared error. This tradeoff is vital for model accuracy.\nLarge Datasets: As datasets grow in size and complexity, shrinkage methods become increasingly relevant. They are designed to handle situations where the number of predictors can exceed the number of observations.\nImportance of Scaling: Unlike least squares, the performance of Ridge regression is sensitive to the scale of the predictors. Standardizing variables ensures comparability and effectiveness of the shrinkage.\nContinuous Shrinkage: Ridge regression produces coefficients that are close to zero but rarely exactly zero, which differs from Lasso. This characteristic can be advantageous for retaining all predictors in the model.\nCurrent Research Trends: Shrinkage methods are a hot topic in statistical research, with ongoing developments aimed at enhancing their effectiveness and applicability across various fields."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Lasso",
    "text": "Summary: Lasso\n\n\nThe Lasso regression technique improves upon ridge regression by both shrinking coefficients and performing variable selection, setting some coefficients to zero.\n\n\nHighlights\n\nLasso regression shrinks coefficients while allowing for variable selection.\nIt uses an \\(L_1\\) penalty, contrasting with ridge’s \\(L_2\\) penalty.\nThe concept of sparsity is central to Lasso’s effectiveness.\nIncreased computational efficiency has popularized Lasso in recent years.\nLasso is particularly useful in high-dimensional datasets with many features.\nCross-validation is essential for selecting the optimal lambda value.\nPerformance varies: Lasso excels in sparse models, while ridge may perform better in dense ones.\n\n\nKey Insights\n\nLasso vs. Ridge: Lasso regression not only shrinks coefficients but also sets some to zero, enabling simpler models through variable selection. This property makes it particularly valuable in high-dimensional settings where many variables may be irrelevant.\n\\(L_1\\) vs. \\(L_2\\) Penalty: The \\(L_1\\) penalty used in Lasso creates a constraint that promotes sparsity, while the \\(L_2\\) penalty in ridge regression tends to retain all variables with smaller coefficients. This difference is crucial for effective model building.\nSparsity: The concept of sparsity refers to models that only include a small subset of variables. Sparse models are easier to interpret and can enhance predictive performance when only a few predictors are relevant.\nComputational Advances: Recent improvements in computational power and techniques in convex optimization have made applying Lasso feasible even on large datasets, broadening its applicability across various fields.\nReal-World Applications: In situations like medical diagnostics, where finding a minimal number of significant predictors is vital, Lasso provides a practical solution by efficiently identifying key variables among thousands of measurements.\nChoosing Lambda: The tuning parameter lambda is critical; cross-validation is typically used to determine its optimal value, balancing model complexity and predictive accuracy.\nModel Performance: The effectiveness of Lasso and ridge regression varies based on the underlying data structure. Lasso performs better with sparse true models, while ridge regression may be more effective when many predictors are significant."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Selecting the tuning parameter (lambda)",
    "text": "Summary: Selecting the tuning parameter (lambda)\n\n\nSelecting the tuning parameter (lambda) for ridge regression and lasso is crucial, as it significantly influences model performance. Cross-validation is an effective method for this selection.\n\n\nHighlights\n\nLambda is crucial: Affects the solution from full least squares to zero coefficients.\n⚖️ Regularization importance: Zero lambda means no regularization; high lambda leads to zero solutions.\nCross-validation advantage: Ideal for tuning parameters as it doesn’t require the unknown number of parameters (\\(d\\)).\nRidge example: With lambda of 100, all variables appear included, but coefficients are shrunk.\nCross-validation curves: Show how errors change with varying lambda values for both ridge and lasso.\nLasso effectiveness: Properly identifies non-zero coefficients while setting others to zero.\nSimulation success: In a simulated scenario, the model accurately identifies the correct number of non-zero coefficients.\n\n\nKey Insights\n\nImportance of Lambda: The tuning parameter lambda significantly influences the model’s complexity and overall performance. Choosing lambda wisely is essential for achieving the desired balance between bias and variance.\nCross-validation as a solution: Cross-validation provides a robust framework for assessing model performance across different lambda values without needing to know the exact number of parameters, making it a practical choice for tuning.\nDegree of freedom confusion: In ridge regression, even when coefficients are shrunk, counting parameters can be misleading, as all variables remain included in the model.\nRegularization trade-offs: The process of regularization through ridge and lasso not only simplifies models but also introduces nuanced definitions of model complexity, changing our understanding of ‘degrees of freedom.’\nError analysis via curves: Cross-validation curves reveal how model errors fluctuate with lambda, helping visualize optimal tuning points.\nLasso’s precision: Lasso regression demonstrates its strength in feature selection, effectively pinpointing relevant variables while ignoring the irrelevant ones, enhancing interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-dimension-reduction",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-dimension-reduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Dimension reduction",
    "text": "Summary: Dimension reduction\n\n\nDimension reduction transforms original predictors into fewer linear combinations, improving model fitting while maintaining low bias and variance.\n\n\nHighlights\n\nDimension Reduction simplifies models by using fewer predictors.\nNew predictors are linear combinations of original ones.\nFitting uses least squares on transformed predictors.\nAim: Reduce dimensions from \\(P\\) predictors to \\(M\\) (\\(M &lt; P\\)).\nBalances bias and variance effectively.\nSimilar to Ridge and Lasso, but with different coefficient constraints.\nWorks best when \\(M &lt; P\\); otherwise, it results in standard least squares.\n\n\nKey Insights\n\nEfficiency in Modeling: Dimension reduction allows for a simpler model with fewer predictors, leading to potentially better performance without losing significant information. This method is advantageous in high-dimensional datasets.\nConstruction of New Predictors: By creating new predictors through linear combinations, we can capture essential relationships in the data while reducing complexity, which may help in enhancing interpretability.\nBias-Variance Trade-off: This approach effectively manages the bias-variance trade-off, leading to models with lower bias and variance compared to using all original features, which is crucial for better generalization to unseen data.\nUse of Least Squares: While retaining the least squares fitting method, this approach modifies the predictor space, allowing for a fresh perspective on regression problems and leading to potentially improved outcomes.\nRelation to Ridge and Lasso: Although dimension reduction shares similarities with Ridge and Lasso in terms of model fitting, it introduces unique constraints on coefficients, which can lead to different insights about the data.\nImportance of Dimensions: The effectiveness of dimension reduction hinges on the condition that \\(M\\) (new predictors) is less than \\(P\\) (original predictors). If \\(M = P\\), the method reduces to standard least squares, negating its advantages.\nInnovation in Coefficient Form: The requirement for coefficients to adopt a specific structure in dimension reduction can provide insights into the relationships among predictors, enhancing model interpretability and utility."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-principal-components-regression-pcr",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-principal-components-regression-pcr",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Principal Components Regression (PCR)",
    "text": "Summary: Principal Components Regression (PCR)\n\n\nPrincipal Components Regression (PCR) reduces dimensionality by finding principal components and using them in least squares regression for efficient modeling.\n\n\nHighlights\n\nPrincipal Components Regression (PCR) uses a two-step procedure to reduce dimensionality.\nFirst, principal components with the highest variance are identified from the data.\nThe first principal component is aligned with the direction of maximum variance.\nThe second principal component is uncorrelated with the first and captures additional variance.\nUsing few principal components can effectively summarize complex datasets.\nChoosing the optimal number of components is crucial for minimizing mean squared error.\nPartial Least Squares (PLS) improves upon PCR by considering response variables in component selection.\n\n\nKey Insights\n\nDimensionality Reduction: PCR simplifies models by reducing the number of predictors while retaining essential information, aiding in interpretation and computation. This is particularly useful with datasets containing many variables relative to observations.\nUncorrelated Components: The process ensures that the principal components are uncorrelated, which helps in creating more robust models by minimizing multicollinearity issues common in regression analysis.\nModel Selection: The selection of the number of components directly impacts model performance. Cross-validation is recommended to find the optimal number of components for the best predictive accuracy.\nEfficiency in Prediction: PCR can significantly enhance prediction accuracy when dealing with high-dimensional data by focusing on variance rather than individual variable contributions.\nAssumption of Variance-Response Relationship: The effectiveness of PCR hinges on the assumption that high variance directions in predictors correlate with the response, which may not always hold true.\nPartial Least Squares: PLS offers a supervised alternative to PCR by incorporating response variable information, potentially leading to better predictive models, although it may not always outperform PCR.\nModern Applications: Techniques like PCR and PLS are increasingly relevant in fields with large datasets, where simpler models are needed to prevent overfitting and enhance interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nNotice that from definition (1),\n\n\\[\n    \\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where\n\\[\n    \\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\n\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  }
]