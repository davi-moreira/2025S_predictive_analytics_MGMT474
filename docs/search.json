[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidesbook lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidesbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidesbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslides - TBPbook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslides - TBPbook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslides - TBPbook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslides - TBPbook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nSupport Vector Machines\nCh. 9\nslides - TBPbook lab\n- Video: Statistical Learning: 9.1 Optimal Separating Hyperplane- Video: Statistical Learning: 9.2.Support Vector Classifier- Video: Statistical Learning: 9.3 Feature Expansion and the SVM- Video: Statistical Learning: 9.4 Example and Comparison with Logistic Regression- Video: Statistical Learning: 9.Py Support Vector Machines I 2023- Video: Statistical Learning: 9.Py ROC Curves I 2023\n\n\nWeek 09\nUnsupervised Learning\nCh. 12\nslides - TBPbook lab\n- Video: Statistical Learning: 12.1 Principal Components- Video: Statistical Learning: 12.2 Higher order principal components- Video: Statistical Learning: 12.3 k means Clustering- Video: Statistical Learning: 12.4 Hierarchical Clustering- Video: Statistical Learning: 12.5 Matrix Completion- Video: Statistical Learning: 12.6 Breast Cancer Example- Video: Statistical Learning: 12.Py Principal Components I 2023- Video: Statistical Learning: 12.Py Clustering I 2023- Video: Statistical Learning: 12.Py Application: NCI60 Data I 2023\n\n\nWeek 10\nFinal Project\n.\n.\n.\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nDeep Learning\nCh. 10\nslides - TBPbook lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 14\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\n\n* The course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nStructured Data\nUnstructured Data\nDatabases\nRelational Databases\n\n\n\nNon Relational Databases\nMeta Data and Dictionary (code book)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is Data?",
    "text": "What is Data?\n\n\n\n\nData refers to raw, unprocessed facts, figures, and symbols that represent information about the world around us. Data can take many forms, such as numbers, text, images, audio, and video, and it can be quantitative (numerical) or qualitative (categorical).\n\n\n\nData (Wiki)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Types of Data",
    "text": "Types of Data\n\n\nStructured Data: Data that is organized in a defined format, such as rows and columns in a database (e.g., an Excel spreadsheet).\nUnstructured Data: Data that does not have a predefined structure, such as text, emails, social media posts, videos, and images.\nSemi-Structured Data: Data that does not conform to a strict structure but contains tags or markers to separate elements (e.g., XML or JSON files)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Structured Business Data",
    "text": "Structured Business Data\n\n\nBusiness data refers to the information gathered by an organization, such as customer data, financial data, sales data, employee data, and more. Business data can come from a wide variety of sources - from customers’ purchase transactions and social media activities to market research and financial reports.\n\nBecause structured data is typically organized in a specific format that can be easily searched and analyzed, most business analytics are designed and applied to structured data. This course will focus solely on using and analyzing structured data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Introduction to Databases",
    "text": "Introduction to Databases\n\n\n\n\nA Database is a structured collection of data, typically managed by a Database Management System (DBMS) to efficiently store, retrieve, and manage data for various applications."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Relational Model: 1970s",
    "text": "The Relational Model: 1970s\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdgar F. Codd\n\n\n\n\nEdgar F. Codd: Proposed the relational model in 1970, which became the foundation for modern databases.\nRDBMS: A Relational Database Management System (RDBMS) is used to maintain relational databases.\nSQL: Structured Query Language (SQL) was developed to interact (query and update) with relational databases.\nAdoption: The relational model became dominant in the 1980s, with systems like Oracle, IBM DB2, and Microsoft SQL Server emerging. Nowadays, open-source systems like MySQL are used by big companies to handle their relational data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Rise of NoSQL: 2000s",
    "text": "The Rise of NoSQL: 2000s\n\nLimitations of RDBMS: Traditional relational databases struggled with the scale and complexity of modern web applications.\n\nExample: A social media platform with millions of users posting, commenting, and liking content simultaneously.\nLimitation: RDBMS typically scale vertically (adding more power to a single server), which becomes increasingly expensive and challenging as the database grows. In contrast, NoSQL databases like Cassandra or MongoDB are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nNoSQL Databases: Emerged to address these challenges. They offer flexibility, scalability, and performance improvements. They are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nTypes: Document (e.g., MongoDB), Key-Value (e.g., Redis), Column-Family (e.g., Cassandra), and Graph (e.g., Neo4j).\nUse Cases: Ideal for big data, handling unstructured data, real-time web applications, and distributed systems."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Modern Database Trends",
    "text": "Modern Database Trends\n\n\nNewSQL: Combines the scalability of NoSQL with the Atomicity, Consistency, Isolation, and Durability (ACID) guarantees of traditional relational databases (e.g., Google Spanner).\nCloud Databases: The adoption of cloud computing has led to the rise of managed database services (e.g., Amazon RDS, Google Cloud SQL).\nData Lakes: A storage repository that holds vast amounts of raw data in its native format (e.g., AWS S3, Azure Data Lake)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA relational database links data tables through pre-defined and shared fields in various data tables, establishing relationships.\nThis permits more efficient organization and utilization of data across multiple tables.\nMoreover, a relational database serves as a potent tool for handling extensive data volumes and managing complex data structures."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 1: Flights from a Specific Carrier\n\n\nSELECT flights.year, flights.month, flights.day, \n       flights.flight, airlines.names AS airline_name\nFROM flights\nJOIN airlines ON flights.carrier = airlines.carrier\nWHERE airlines.names = 'American Airlines';  \n-- Replace 'American Airlines' with the desired carrier name"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 2: Weather Conditions at the Time of a Specific Flight\n\n\nSELECT flights.flight, flights.origin, \n        flights.dest, weather.*\nFROM flights\nJOIN weather \nON flights.year = weather.year AND\n   flights.month = weather.month AND\n   flights.day = weather.day AND\n   flights.hour = weather.hour AND\n   flights.origin = weather.origin\nWHERE flights.flight = 'AA123';  \n-- Replace 'AA123' with the desired flight number"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 3: Count of Flights Per Airport\n\n\n\nSELECT airports.faa, \n        COUNT(flights.flight) AS flight_count\nFROM flights\nJOIN airports ON flights.origin = airports.faa\nGROUP BY airports.faa\nORDER BY flight_count DESC;"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse",
    "text": "Data Warehouse\n\n\nA Data Warehouse is a large and comprehensive storage system that consolidates data from various sources, including relational databases, into a centralized repository, much like a university campus that encompasses buildings of various functions.\n\nThe primary purpose of a data warehouse is to facilitate data storage, reporting, and analysis for business intelligence and decision-making purposes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nThe primary goal of an Information Management System (IMS) is to ensure that accurate, timely, and relevant information is generated and available to the right people at the right time, enabling efficient and informed decision-making processes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nKey Components: Data sources, ETL (Extract, Transform, Load), Data Warehouse, OLAP Engine, and Analytic Reporting."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nFinance Data: Information related to financial transactions, budgeting, and accounting.\nCRM Data: Customer Relationship Management data, including customer interactions, sales, and service records.\nOperations Data: Data concerning the day-to-day operations of a business, such as supply chain, inventory, and production.\nMore Data: Any additional data sources that contribute to the organization’s information ecosystem."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "ETL Process (Extract, Transform, Load)",
    "text": "ETL Process (Extract, Transform, Load)\n\n\nExtract: Data is collected from various sources, such as finance systems, CRM systems, and operations databases.\nTransform: The extracted data is cleaned, aggregated, and formatted to fit the data warehouse schema.\nLoad: The transformed data is loaded into the Data Warehouse, where it is stored and made available for analysis."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse and OLAP Engine",
    "text": "Data Warehouse and OLAP Engine\n\n\nData Warehouse: A centralized repository that stores integrated data from multiple sources, optimized for query and analysis.\nOLAP Engine (Online Analytical Processing): Tools that allow for complex analytical queries and multi-dimensional data analysis.\n\nExample: Analyzing sales trends over time, across different regions, or by product categories."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Analytic Reporting and Advanced Analytics",
    "text": "Analytic Reporting and Advanced Analytics\n\n\nAnalytic Reporting Engine: Produces reports and dashboards for users to visualize and understand the data.\n\nAd Hoc Reporting: Enables users to create custom reports on-demand.\nDashboards: Provides a visual summary of key performance indicators (KPIs) and metrics.\n\nAdvanced Analytics: Includes data mining, predictive modeling, and other sophisticated analytical techniques to uncover hidden patterns and insights."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Users and Decision-Making",
    "text": "Users and Decision-Making\n\n\nUsers: Business analysts, managers, and executives who use the IMS to make informed decisions.\nOutcome: The IMS enables data-driven decision-making, improving efficiency, reducing risks, and enhancing overall business performance."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Meta Data and Data Dictionary",
    "text": "Meta Data and Data Dictionary\n\n\n\nMetadata is essentially information about structured data. It can include details like the date and time a database or file was created, who created it, and what types of information it contains.\nA data dictionary is a more specific type of metadata that describes the structure, content, and format of a dataset. It’s like a guidebook and a codebook that provides a comprehensive list of all the variables or columns in a dataset, along with their definitions, data types, and other attributes.\n\nWithout it, you might get lost in a sea of information and struggle to make sense of it all."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "mtcars",
    "text": "mtcars\n\n\n\nThe mtcars data file provides information on various features of different brands of cars, including their engine size, horsepower, and fuel efficiency. The dataset is structured as a table, where each row represents a different car, and each column represents a different variable or feature of the cars."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Dictionary for mtcars",
    "text": "Data Dictionary for mtcars\n\n\n?mtcars"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Learning Path",
    "text": "Learning Path\nThere are plenty of college courses to choose (course titles may vary by schools):\n\nDatabase Management Systems: This course focuses on the design, implementation, and management of databases, teaching students how to organize and manage data effectively.\nInformation Security and Privacy: This course covers the principles and practices of securing information and ensuring data privacy, preparing students to handle data security challenges.\nData Governance and Management: This course explores the governance and management of data assets, including data quality, data integration, and data lifecycle management.\nInformation Systems Analysis and Design: This course teaches students how to analyze business requirements and design information systems to meet organizational needs."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nStructured Data: Highly organized and formatted data that is easily searchable (e.g., tables with rows and columns).\nDatabases: Used to store and manage structured data efficiently.\n\nTypes: Include relational databases (RDBMS) like MySQL and NoSQL databases like MongoDB.\nFunctionality: Provides tools for querying, updating, and managing large datasets.\n\nRelational Databases: Organizes data into tables that can be linked by shared keys.\n\nBenefits: Ensures data integrity and supports complex queries and transactions.\nKey Components: Tables, primary and foreign keys, SQL for data manipulation.\n\n\n\n\n\n\nNon-Relational Databases: NoSQL databases designed for unstructured data and scalability.\n- Advantages: Handle large-scale data across distributed systems more effectively than traditional RDBMS.\nMeta Data: Information describing other data, providing context and making it easier to understand.\nData Dictionary: Detailed description of dataset variables, ensuring consistent data usage."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n20%\n\n\nHomework\n30%\n\n\nFinal Project\n40%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (40%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPoster Presentation at the Purdue Undergraduate Research Conference (40%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nIt is your turn! - 5 minutes\n\n\n\nPresent yourself to your left/right colleague and tell her/him what are the current two main passions in your life."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\nMaterials:\n\nBrightspace\nCourse Webpage\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples.\nObjective is more fuzzy:\n\nFind groups of samples that behave similarly.\nFind features that behave similarly.\nFind linear combinations of features with the most variation.\n\nDifficult to know how well we are doing.\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nQuestions we might ask:\n\n\n\n\nIs there a relationship between advertising budget and sales?\nHow strong is the relationship between advertising budget and sales?\nWhich media contribute to sales?\nHow accurately can we predict future sales?\nIs the relationship linear?\nIs there synergy among the advertising media?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values (e.g., gender, ethnicity).\nCategorical predictors can be represented using factor variables.\nQualitative variables: Gender, Student (Student Status), Status (Marital Status), Ethnicity."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between males and females, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person is female} \\\\\n0 & \\text{if } i\\text{th person is male}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person is female} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person is male.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for gender model:\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nGender \\(Female\\)\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, contrary to popular wisdom, females don’t generally have a higher credit card balance than males."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the ethnicity variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is Asian} \\\\\n      0 & \\text{if i-th person is not Asian}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is Caucasian} \\\\\n      0 & \\text{if i-th person is not Caucasian}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is Asian} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is Caucasian}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is African American (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(Asian\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(Caucasian\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares Asian to African American and that’s not significant. Likewise, the Caucasian to African-American is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Linear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#overview",
    "href": "lecture_slides/04_classification/04_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Classification\nLinear versus Logistic Regression\nMaking Predictions\nMultinomial Logistic Regression\n\n\n\nDiscriminant Analysis\nLinear Discriminant Analysis when \\(p &gt; 1\\)\nTypes of errors\nOther Forms of Discriminant Analysis\nNaive Bayes\nGeneralized Linear Models"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classification",
    "href": "lecture_slides/04_classification/04_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "href": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression: Probability of Default",
    "text": "Linear versus Logistic Regression: Probability of Default\n\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\nLogistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "href": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\n\n\n\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant Euler’s number.\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\n\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 1: Express \\(1 - p(X)\\)\n\nSince \\(p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\), we can write:\n\\[\n1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\nSimplify:\n\\[\n1 - p(X) = \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 2: Compute the Odds\n\nThe odds are defined as:\n\\[\n\\frac{p(X)}{1 - p(X)}\n\\]\nSubstitute \\(p(X)\\) and \\(1 - p(X)\\):\n\\[\n\\frac{p(X)}{1 - p(X)} =\n\\frac{\\dfrac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}\n{\\dfrac{1}{1 + e^{\\beta_0 + \\beta_1 X}}}\n\\]\nSimplify:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 3: Take the Log of the Odds\n\nTaking the natural logarithm:\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\log\\!\\Bigl(e^{\\beta_0 + \\beta_1 X}\\Bigr)\n\\]\nSimplify using the log property \\(\\log(e^x) = x\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nConclusion\n\nThe final transformation shows that the log-odds (logit) of \\(p(X)\\) is a linear function of \\(X\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a model by maximizing the likelihood function, which measures how likely the observed data is given the parameters.\n\nThe likelihood function is based on the probability distribution of the data. If you assume that the data points are independent, the likelihood function is the product of the probabilities of each observation.\n\nConsidering a data series of observed zeros and ones, and a model for the probabilities involving parameters (e.g., \\(\\beta_0\\) and \\(\\beta_1\\)), for any specific parameter values, we can compute the probability of observing the data.\nSince the observations are assumed to be independent, the joint probability of the observed sequence is the product of the probabilities for each observation. For each “1,” we use the model’s predicted probability, \\(p(x_i)\\), and for each “0,” we use \\(1 - p(x_i)\\).\nThe goal of MLE is to find the parameter values that maximize this joint probability, as they make the observed data most likely to have occurred."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSuppose you are flipping a coin, and you observe 5 heads out of 10 flips. The coin’s bias (the probability of heads) is \\(p\\), and you want to estimate \\(p\\).\nThe probability of observing a single outcome (heads or tails) follows the Bernoulli distribution:\n\\[\nP(\\text{Heads or Tails}) = p^x (1-p)^{1-x}, \\quad \\text{where } x = 1 \\text{ for heads, } x = 0 \\text{ for tails.}\n\\]\nFor 10 independent flips, the likelihood function is:\n\\[\nL(p) = P(\\text{data} \\mid p) = \\prod_{i=1}^{10} p^{x_i}(1-p)^{1-x_i}.\n\\]\nIf there are 5 heads (\\(x=1\\)) and 5 tails (\\(x=0\\)):\n\\[\nL(p) = p^5 (1-p)^5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSimplify with the Log-Likelihood\nSince multiplying probabilities can result in very small numbers, we take the logarithm of the likelihood (log-likelihood). The logarithm simplifies the product into a sum:\n\\[\n\\ell(p) = \\log L(p) = \\log \\left(p^5 (1-p)^5\\right) = 5\\log(p) + 5\\log(1-p).\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nMaximize the Log-Likelihood\nTo find the value of \\(p\\) that maximizes \\(\\ell(p)\\), take the derivative of the log-likelihood with respect to \\(p\\) and set it to zero:\n\\[\n\\frac{\\partial\\ell(p)}{\\partial p} = \\frac{5}{p} - \\frac{5}{1-p} = 0.\n\\]\nSimplify:\n\\[\n\\frac{5}{p} = \\frac{5}{1-p}.\n\\]\nSolve for \\(p\\):\n\\[\n1 - p = p \\quad \\Rightarrow \\quad 1 = 2p \\quad \\Rightarrow \\quad p = 0.5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nTo confirm that \\(p = 0.5\\) is the maximum, you can check the second derivative of the log-likelihood (concavity) or use numerical methods.\nIn our example, \\(p = 0.5\\) makes sense intuitively because the data (5 heads out of 10 flips) suggests the coin is unbiased.\nThe maximum likelihood estimate of \\(p\\) is \\(0.5\\). The MLE method finds the parameter values that make the observed data most likely, given the assumed probability model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nAssumptions:\n\nData \\(x_1, x_2, \\dots, x_n\\) are drawn from a normal distribution with:\n\n\\[\n  f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\n\nAssume \\(\\sigma\\) is known (say, \\(\\sigma = 1\\)) and we want to estimate \\(\\mu\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\nThe likelihood for \\(n\\) independent observations is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x_i - \\mu)^2}{2}}\n\\]\nTaking the natural log:\n\\[\n\\ell(\\mu) = \\log L(\\mu) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2} \\right]\n\\]\nSimplify (since \\(-\\frac{1}{2} \\log(2\\pi)\\) is constant):\n\\[\n\\ell(\\mu) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = -\\sum_{i=1}^n (x_i - \\mu)\n\\] Set this to zero:\n\\[\n\\sum_{i=1}^n (x_i - \\mu) = 0\n\\]\nSolve for \\(\\mu\\):\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nThe MLE for the mean \\(\\mu\\) is simply the sample mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "href": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nMost statistical packages can fit linear logistic regression models by maximum likelihood.\nLogistic Regression Coefficients\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.6513\n0.3612\n-29.5\n&lt; 0.0001\n\n\nbalance\n0.0055\n0.0002\n24.9\n&lt; 0.0001\n\n\n\n\nWhat is our estimated probability of default for someone with a credit card balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a a credit card balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent \\(Yes\\)\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\n\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent Yes\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding",
    "href": "lecture_slides/04_classification/04_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "href": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\text{Softmax}(z_k) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nWe really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example will given later in the course, when we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the Softmax Function?",
    "text": "What is the Softmax Function?\n\n\nThe softmax function is used in multinomial logistic regression to convert raw scores (logits) into probabilities for multiple classes.\n\n\nLogits are the raw, untransformed output of the linear component in logistic regression. For a given class \\(k\\), the logit is defined as:\n\\[\nz_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\n\\]\nWhere:\n\n\\(z_k\\): The logit for class \\(k\\).\n\\(\\beta_{k0}\\): Intercept term.\n\\(\\beta_{kj}\\): Coefficients for predictor \\(x_j\\).\n\n\n\nSoftmax Definition:\nFor \\(K\\) classes and input \\(x\\), the softmax function is defined as:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}}\n\\]\nWhere:\n\n\\(z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\\): The linear score (logit) for class \\(k\\).\n\\(\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp}\\): Coefficients for class \\(k\\).\n\\(e^{z_k}\\): Exponentiated score for class \\(k\\), ensuring all values are positive.\n\n\n\n\nKey Features of the Softmax Function\n\nProbability Distribution: Outputs probabilities that sum to 1 across all \\(K\\) classes. \\(\\text{Pr}(Y = k \\mid X = x) = \\text{Softmax}(z_k)\\).\nNormalization: Normalizes logits by dividing each exponentiated logit by the sum of all exponentiated logits.\nHandles Multiclass Classification: Extends binary logistic regression to \\(K &gt; 2\\) classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "href": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example of Softmax in Action",
    "text": "Example of Softmax in Action\n\n\n\n\nImagine classifying three emergency room conditions: Stroke, Drug Overdose, and Epileptic Seizure.\nSuppose the logits are: \\(z_{\\text{stroke}} = 2.5, \\quad z_{\\text{drug overdose}} = 1.0, \\quad z_{\\text{epileptic seizure}} = 0.5\\)\nThe probabilities are:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{e^{2.5} + e^{1.0} + e^{0.5}}\n\\]\n\nStep 1: Exponentiate the Logits\n\\(e^{z_{\\text{stroke}}} = e^{2.5} \\approx 12.182\\)\n\\(e^{z_{\\text{drug overdose}}} = e^{1.0} \\approx 2.718\\)\n\\(e^{z_{\\text{epileptic seizure}}} = e^{0.5} \\approx 1.649\\)\n\n\nStep 2: Compute the Denominator\n\\(\\sum_{l=1}^K e^{z_l} = e^{2.5} + e^{1.0} + e^{0.5}\\)\n\\(\\sum_{l=1}^K e^{z_l} \\approx 12.182 + 2.718 + 1.649 = 16.549\\)\n\n\n\nStep 3: Calculate the Probabilities\n\\(\\text{Pr}(\\text{stroke}) = \\frac{e^{z_{\\text{stroke}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{12.182}{16.549} \\approx 0.7366\\)\n\\(\\text{Pr}(\\text{drug overdose}) = \\frac{e^{z_{\\text{drug overdose}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{2.718}{16.549} \\approx 0.1642\\)\n\\(\\text{Pr}(\\text{epileptic seizure}) = \\frac{e^{z_{\\text{epileptic seizure}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{1.649}{16.549} \\approx 0.0996\\)\nThe output probabilities represent the likelihood of each condition, ensuring:\n\\[\n\\sum_{k=1}^3 \\text{Pr}(Y = k) = 1\n\\]\nWe have:\n\\[\n   0.7366 + 0.1642 + 0.0996 \\approx 1.000\n\\]\n\n\nConclusion\n\nThe softmax function translates raw scores into probabilities, making it essential for multiclass classification.\nIt ensures a probabilistic interpretation while maintaining normalization across all classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions as input for \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes’ Theorem: Explanation",
    "text": "Bayes’ Theorem: Explanation\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A|B)\\): Posterior probability - Probability of event \\(A\\) occurring given that \\(B\\) is true — updated probability after the evidence is considered.\n\\(P(A)\\): Prior probability - Initial probability of event \\(A\\) — the probability before the evidence is considered.\n\\(P(B|A)\\): Likelihood - Probability of observing event \\(B\\) given that \\(A\\) is true.\n\\(P(B)\\): Marginal probability - Total probability of the evidence, event \\(B\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "href": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Understanding Conditional Probability",
    "text": "Understanding Conditional Probability\nConditional probability is the probability of an event occurring given that another event has already occurred.\nDefinition:\n\\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nis the probability of event \\(A\\) occurring given that \\(B\\) is true.\n\nInterpretation: How likely is \\(A\\) if we know that \\(B\\) happens?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Joint Probability?",
    "text": "What is Joint Probability?\nJoint probability refers to the probability of two events occurring together.\nDefinition: \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur.\n\nConnection to Conditional Probability:\n\\[\n  P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n  P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\nThis formula is crucial for understanding Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "href": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Symmetry in Joint Events",
    "text": "Symmetry in Joint Events\nJoint probability is symmetric, meaning:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, we can also express it as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\nThis symmetry is the key to deriving Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "href": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deriving Bayes’ Theorem",
    "text": "Deriving Bayes’ Theorem\n\n\nGiven that the definition of Conditional Probability is:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\nUsing the Definition of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n   P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\n\n\n\nSymmetry of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(B \\cap A)\n\\]\n\n\nThus, we can express the joint probability as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\n\nThe Bayes’ Theorem!\n\nSubstitute this back into the conditional probability definition:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "href": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Bayes’ Theorem Matters?",
    "text": "Why Bayes’ Theorem Matters?\n\n\nBayes’ Theorem is a foundational principle in probability theory and statistics, enabling:\n\nIncorporation of Prior Knowledge:\nIt allows for the integration of prior knowledge or beliefs when making statistical inferences.\nBeliefs Update:\nIt provides a systematic way to update the probability estimates as new evidence or data becomes available.\nProbabilistic Thinking:\nEncourages a probabilistic approach to decision-making, quantifying uncertainty, and reasoning under uncertainty.\nVersatility in Applications:\nFrom medical diagnosis to spam filtering, Bayes’ Theorem is pivotal in areas requiring probabilistic assessment.\n\nBayes’ Theorem is a paradigm that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\nLeft-hand plot: single variable X and \\(\\pi_k f_k(x)\\) in the vertical axis for both classes \\(k\\) equals 1 and \\(k\\) equals 2. In this case the the pies are the same for both, so anything to the left of zero we classify as as green and anything to the right we classify as as purple.\nRight-hand plot: here we have different priors. The probability of \\(k = 2\\) is 0.7 and and of of \\(k= 1\\) is 0.3. The decision boundary moved slightly to the left. On the right, we favor the pink class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\n\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify one observation at the value \\(X = x\\) into a class, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nLeft-Panel: Synthetic population data with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule.\nRight-Panel: histograms of the sample. We see that the estimation provided a decision boundary (black solid line) pretty close to the correct one, the one of the population."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "href": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\n\nThe prior is the number in each class divided by the total number:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nThe means in each class is the sample mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\nWe assume that the variance is the same in each of the classes and so we assume a pooled variance estimate:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nGaussian density in two Dimensions, two variables \\(x_1\\) and \\(x_2\\). On the Left-panel, we have a bell function and this is the case when the two variables are uncorrelated. On the Right-panel, there is correlation between the two predictors and it is like a stretched bell.\nDensity:\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\] where \\(\\Sigma\\) is the covariance matrix."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\n\n\nThe covariance matrix is a square matrix that summarizes the covariance (a measure of how much two random variables vary together) between multiple variables in a dataset.\nDefinition:\nFor a random vector \\(X = [X_1, X_2, \\dots, X_p]^\\top\\) with \\(p\\) variables, the covariance matrix \\(\\Sigma\\) is defined as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n\\end{bmatrix}\n\\]\n\nKey Properties:\n\n\\(\\text{Var}(X_i)\\): Variance of variable \\(X_i\\).\n\\(\\text{Cov}(X_i, X_j)\\): Covariance between variables \\(X_i\\) and \\(X_j\\).\n\\(\\Sigma\\) is symmetric: \\(\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)\\).\nDiagonal elements represent variances, and off-diagonal elements represent covariances."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nDiscriminant function: after simplifying the density function we can find\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\]\nNote that it is a linear function where the first component, \\(x^T \\Sigma^{-1} \\mu_k\\), has the \\(x\\) variable multiplied by a coefficient vector and, the second component, \\(\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\), is a constant."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nThe Discriminant function can be written as\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\]\na linear function. That is a function for class \\(k\\), where \\(c_{k0}\\) represents the constant we find in the second component of the Discriminant function and \\(c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\) come from the first component of the Discriminant function. We compute \\(\\delta_k(x)\\) for each of the classes and then you classify to the class for which it is largest."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\n\nLeft-panel: The circle presents the countor of the density of a particular level of probability for the blue, green, and the orange class. Here \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\). The dashed lines are known as the Bayes decision boundaries. They are the “True” decision boundaries, were they known, they would yield the fewest misclassification errors, among all possible classifiers.\nRight-panel: We compute the mean for \\(x_1\\) and \\(x_2\\) for the each blue, green, and orange class. After plugging them into the formula, instead of getting the the dotted lines we get the solid black lines."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Iris Data",
    "text": "Example: Fisher’s Iris Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 variables\n3 species\n50 samples/class\n\n🟦 Setosa\n🟧 Versicolor\n🟩 Virginica\n\nLDA classifies all but 3 of the 150 training samples correctly."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Discriminant Plot",
    "text": "Example: Fisher’s Discriminant Plot\n\n\nDiscriminant variables 1 and 2 are linear combinations of the original variables.\nLDA classifies points based on their proximity to centroids in discriminant space.\nThe centroids lie in a subspace of the multi-dimensional space (e.g., a plane within 4D space).\nFor \\(K\\) classes:\n\nLDA can be visualized in \\(K - 1\\)-dimensional space.\nFor \\(K &gt; 3\\), the “best” 2D plane can be chosen for visualization."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\n\n\nOnce we have estimates of the Discriminat Functions, \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "href": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Default Status\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\n\n\n\n\\(\\frac{23 + 252}{10000}\\) errors — a 2.75% misclassification rate!\n\n\nSome caveats:\n\nThis is training error, and we may be overfitting.\nIf we classified to the prior, the proportion of cases in the classes (e.g. always assuming the class No default). We would make \\(\\frac{333}{10000}\\) errors, or only 3.33%. This is what we call the null rate.\nWe can break the errors into different kinds: of the true No’s, we make \\(\\frac{23}{9667} = 0.2\\%\\) errors; of the true Yes’s, we make \\(\\frac{252}{333} = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "href": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "href": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "href": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC plot displays both simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\n\n\n\n\n\n\n\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\nIn the Left-plot we see a case when the true boundary should be linear. In the Right-plot, covariances were different in the true data. It is possible to see that the bayes decision boundary is curved and the quadratic discriminant analysis is also curved whereas the linear discriminant analysis gives a different boundary.\nWhether each class has the same or different covariance matrices significantly impacts how boundaries between the classes are defined. The covariance matrix describes the spread or variability of data points within each class and how the features in that class relate to each other.\n\nKey Insight: If \\(\\Sigma_k\\) are different for each class, the quadratic terms matter significantly.\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nExample: Suppose we are classifying plants based on two features (e.g., height and leaf width). If one type of plant has a tall and narrow spread of data, while another type has a short and wide spread, QDA can handle these differences and draw curved boundaries to separate the groups."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "href": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assess the Covariance Matrices",
    "text": "Assess the Covariance Matrices\n\nLDA assumes the covariance matrices of all classes are the same, while QDA allows each class to have its own. To determine which assumption is better:\n\nHypothesys test: we can perform a Test for Equality of Covariance Matrices (e.g. Box’s M Test). If the covariance matrices are similar (test is not significant): LDA is appropriate. If the covariance matrices differ (test is significant): QDA may be better.\nVisual Inspection: Plot the data in two dimensions (e.g., using scatterplots). Check if the spread, shape, or orientation of data points differs significantly between classes. If they are similar, LDA might work well. If they are visibly different, QDA is likely better.\nCompare Model Performance: run both models and choose the model that performs better on unseen data (test set).\nConsider the Number of Features and Data Size: LDA performs well with smaller datasets because it estimates a single covariance matrix across all classes (fewer parameters). QDA requires a larger dataset because it estimates a separate covariance matrix for each class (more parameters).\nDomain Knowledge: Use your understanding of the data to decide."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\n\nFor a two-class problem, one can show that for LDA:\n\\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\nif we take the log odds, \\(\\log \\left( \\frac{p_1(x)}{1 - p_1(x)}\\right)\\), which is the log of the probability for class 1 versus the probability for class two, we endup with a linear function of \\(x\\), \\(c_0 + c_1 x_1 + \\dots + c_p x_p\\). So it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\). In Machine Learning, it is known as discriminative learning.\nLDA uses the full likelihood based on the joint distributions of \\(x's\\) and \\(y's\\), \\(\\text{Pr}(X, Y)\\), whereas logistic regression was only using the distribution of \\(y's\\). It is known as generative learning.\nDespite these differences, in practice, the results are often very similar.\n\nLogistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes assumes each \\(\\Sigma_k\\) is diagonal:\n\\[\n\\begin{aligned}\n\\delta_k(x) &\\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\end{aligned}\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\nKey Point: Despite strong assumptions, naive Bayes often produces good classification results.\n\nExplanation:\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_{kj}(x_j)\\): Density function for feature \\(j\\) in class \\(k\\).\n\\(\\mu_{kj}\\): Mean of feature \\(j\\) in class \\(k\\).\n\\(\\sigma_{kj}^2\\): Variance of feature \\(j\\) in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diagonal Covariance Matrix",
    "text": "Diagonal Covariance Matrix\n\n\nA diagonal covariance matrix is a special case of the covariance matrix where all off-diagonal elements are zero. This implies that the variables are uncorrelated.\nGeneral Form:\nFor \\(p\\) variables, a diagonal covariance matrix \\(\\Sigma\\) is represented as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\nProperties:\n\nDiagonal Elements (\\(\\sigma_i^2\\)): Represent the variance of each variable \\(X_i\\).\nOff-Diagonal Elements: All equal to zero (\\(\\text{Cov}(X_i, X_j) = 0\\) for \\(i \\neq j\\)), indicating no linear relationship between variables.\nA diagonal covariance matrix assumes independence between variables. Each variable varies independently without influencing the others.\nCommonly used in simpler models, such as Naive Bayes, where independence is assumed."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nUseful if some classes are well separated. A situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example: \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\\) We estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis toy example demonstrates the working of the Naïve Bayes classifier for two classes (\\(k = 1\\) and \\(k = 2\\)) and three features (\\(X_1, X_2, X_3\\)). The goal is to compute the posterior probabilities \\(\\Pr(Y = 1 \\mid X = x^*)\\) and \\(\\Pr(Y = 2 \\mid X = x^*)\\) for a given observation \\(x^* = (0.4, 1.5, 1)\\).\n\nThe prior probabilities for each class are:\n\\(\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\\)\n\n\nFor each feature (\\(X_1, X_2, X_3\\)), we estimate the class-conditional density functions:\n\n\\(\\hat{f}_{11}, \\hat{f}_{12}, \\hat{f}_{13}\\): Densities for \\(k = 1\\) (class 1).\n\n\\(\\hat{f}_{11}(0.4) = 0.368 \\\\\\)\n\\(\\hat{f}_{12}(1.5) = 0.484 \\\\\\)\n\\(\\hat{f}_{13}(1) = 0.226 \\\\\\)\n\n\\(\\hat{f}_{21}, \\hat{f}_{22}, \\hat{f}_{23}\\): Densities for \\(k = 2\\) (class 2).\n\n\\(\\hat{f}_{21}(0.4) = 0.030 \\\\\\)\n\\(\\hat{f}_{22}(1.5) = 0.130 \\\\\\)\n\\(\\hat{f}_{23}(1) = 0.616 \\\\\\)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\nCompute Class-Conditional Likelihoods for each class \\(k\\), the likelihood is computed as the product of the conditional densities for each feature:\n\n\\[\n   \\hat{f}_k(x^*) = \\prod_{j=1}^3 \\hat{f}_{kj}(x_j^*)\n\\]\n\nFor \\(k = 1\\):\n\n\\[\n     \\hat{f}_{11}(0.4) = 0.368, \\quad \\hat{f}_{12}(1.5) = 0.484, \\quad \\hat{f}_{13}(1) = 0.226\n\\]\n\\[\n     \\hat{f}_1(x^*) = 0.368 \\times 0.484 \\times 0.226 \\approx 0.0402\n\\]\n\nFor \\(k = 2\\):\n\n\\[\n     \\hat{f}_{21}(0.4) = 0.030, \\quad \\hat{f}_{22}(1.5) = 0.130, \\quad \\hat{f}_{23}(1) = 0.616\n\\]\n\\[\n     \\hat{f}_2(x^*) = 0.030 \\times 0.130 \\times 0.616 \\approx 0.0024\n\\]\n\n\n\nCompute Posterior Probabilities using Bayes’ theorem:\n\n\\[\n   \\Pr(Y = k \\mid X = x^*) = \\frac{\\hat{\\pi}_k \\hat{f}_k(x^*)}{\\sum_{k=1}^2 \\hat{\\pi}_k \\hat{f}_k(x^*)}\n\\]\n\nFor \\(k = 1\\): \\[\n\\Pr(Y = 1 \\mid X = x^*) = \\frac{0.5 \\times 0.0402}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.944\n\\]\nFor \\(k = 2\\): \\[\n\\Pr(Y = 2 \\mid X = x^*) = \\frac{0.5 \\times 0.0024}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.056\n\\]\n\n\n\nKey Takeaways:\n\nNaïve Bayes Assumption: The assumption of feature independence simplifies computation by allowing the class-conditional densities to be computed separately for each feature.\nPosterior Probabilities: The posterior probability combines the prior (\\(\\pi_k\\)) and the likelihood (\\(\\hat{f}_k(x^*)\\)).\nClassification: The observation \\(x^*\\) is classified as the class with the highest posterior probability (\\(Y = 1\\))."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\n\nNaïve Bayes classifier can be understood as a special case of a GAM.\n\\[\n\\begin{aligned}\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n&= \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right) \\\\\n&= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\end{aligned}\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model is a Generalized Additive Model (GAM):\n\nThe log-odds are expressed as a sum of additive terms.\n\\(a_k\\): Represents prior influence.\n\\(g_{kj}(x_j)\\): Represents feature contributions."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nLog-Odds of Posterior Probabilities\nThe Naïve Bayes classifier starts with the log-odds of the posterior probabilities:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n\\]\nThis is the log of the ratio of the probabilities of class \\(k\\) and a reference class \\(K\\), given the feature vector \\(X = x\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nBayes’ Theorem\nUsing Bayes’ theorem, the posterior probabilities can be expressed as:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_k(x)\\): Class-conditional density for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nNaïve Bayes Assumption\nThe Naïve Bayes assumption states that features are conditionally independent given the class:\n\\[\nf_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n\\]\nSubstituting this into the equation:\n\\[\n\\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) = \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nSeparate the Terms\nThe terms can now be separated:\n\\[\n\\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\n\\(\\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\): Influence of prior probabilities.\n\\(\\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\): Contribution from each feature."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nAdditive Form\nDefine:\n\\[\na_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right), \\quad g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\nThe equation becomes:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = a_k + \\sum_{j=1}^p g_{kj}(x_j)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\n\n\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "href": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Mean/Variance Relationship",
    "text": "Example: Mean/Variance Relationship\n\n\nLeft plot: we see that the variance mostly increases with the mean.\n10% of a linear model predictions are negative! (not shown here.). However, we know that the response variable, bikers, is always positive.\nTaking log(bikers) alleviates this, but is not a good solution. It has its own problems: e.g. predictions are on the wrong scale, and some counts are zero!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "href": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nPoisson distribution is useful for modeling counts:\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\nMean/variance relationship: \\(\\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\\) i.e., there is a mean/variance dependence. When the mean is higher, the variance is higher.\nModel with Covariates:\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nOr equivalently:\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAutomatic positivity: The model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Poisson Regression on Bikeshare Data",
    "text": "Example: Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote: in this case, the variance is somewhat larger than the mean — a situation known as overdispersion. As a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\n\nWe have covered three GLMs: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\), \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include Gamma, Negative-binomial, Inverse Gaussian, and more."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#summary-1",
    "href": "lecture_slides/04_classification/04_classification.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside [0, 1].\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  }
]